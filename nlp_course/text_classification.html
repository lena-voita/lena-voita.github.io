---
layout: lecture
title: Text Classification
description: Text classification from classical approaches to neural networks.
---


<div class="sidebar" id="sidebar">
    <div class="sidebar_components">
<a href="javascript:void(0)" id="close_sidebar_btn" onclick="closeNav()"
   style="text-align:center;font-size:30px;padding:0px;">⇤</a>
   <a class="active" href="../nlp_course.html" style="font-weight: bold;">
        <img height="18" class='sidebar_ico' src="../resources/lectures/ico/course_logo.png" style="margin-right: 8px;margin-left: 8px;margin-top: 4px;"/>
        NLP Course <font color="#92bf32" id="for_you_in_sidebar">| For You</font></a>
        <a href="#main_content" style="font-weight: bold;">Text Classification</a>
        <a href="#intro">Intro and Datasets</a>


        <div class="dropdown-scope">
           <a class="dropdown-btn">General View
            <i class="fa fa-caret-down"></i>
          </a>
          <div class="dropdown-container">
              <a href="#general_view"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                    Features + Classifier</a>
              <a href="#generative_discriminative"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                  Generative vs Discriminative</a>
          </div>
        </div>


        <div class="dropdown-scope">
           <a class="dropdown-btn" >Classical Methods
            <i class="fa fa-caret-down"></i>
          </a>
          <div class="dropdown-container">
              <a href="#naive_bayes"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                  Naive Bayes</a>
                <a href="#logistic_regression"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                    MaxEnt (Logistic Regression)</a>
              <a href="#svm"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                    SVM</a>
          </div>
        </div>



        <div class="dropdown-scope">
           <a class="dropdown-btn" >Neural Networks
            <i class="fa fa-caret-down"></i>
          </a>
          <div class="dropdown-container">
              <a href="#nn_high_level"><span style="margin-right:15px;font-size:14px;">&#8226;</span>High-Level Pipeline</a>
              <a href="#nn_training"><span style="margin-right:15px;font-size:14px;">&#8226;</span>Training</a>
              <a href="#nn_models"><span style="margin-right:15px;font-size:14px;">&#8226;</span>Models: (Weighted) BOE</a>
              <a href="#nn_models_rnn"><span style="margin-right:15px;font-size:14px;">&#8226;</span>Models: Recurrent</a>
              <a href="#nn_models_cnn"><span style="margin-right:15px;font-size:14px;">&#8226;</span>Models: Convolutional</a>
          </div>
        </div>

        <a href="#multi_label">Multi-Label Classification</a>

         <div class="dropdown-scope">
           <a class="dropdown-btn" >Practical Tips
            <i class="fa fa-caret-down"></i>
          </a>
          <div class="dropdown-container">
              <a href="#practical_tips"><span style="margin-right:15px;font-size:14px;">&#8226;</span>Embeddings: Take Pretrained</a>
              <a href="#data_augmentation"><span style="margin-right:15px;font-size:14px;">&#8226;</span>Data Augmentation</a>


          </div>
        </div>


<a href="#analysis_interpretability" id="sidebar_analysis">Analysis and Interpretability
    <img height="25" src="../resources/lectures/ico/analysis_empty.png" class="sidebar_ico"/></a>
<div class="extra_components">
    <a href="#research_thinking" id="sidebar_research_thinking">Research Thinking
        <img height="30" src="../resources/lectures/ico/bulb_empty.png" class="sidebar_ico"/></a>
    <!--<hr color="#b7db67">-->

    <a href="#related_papers" id="sidebar_related_papers">Related Papers
        <img height="22" src="../resources/lectures/ico/book_empty.png" class="sidebar_ico"/></a>
    <!--<hr color="#b7db67">-->

  <a href="#have_fun" id="sidebar_fun">Have Fun! <img height="30" src="../resources/lectures/ico/fun_empty.png" class="sidebar_ico"/></a>
</div>
</div>
</div>



<div class="sidebar" id="sidebar_small">

  <a class="active" onclick="openNav()" style="text-align:center;">☰</a>
    <a href="../nlp_course.html" style="font-weight: bold;">
        <img height="20" src="../resources/lectures/ico/course_logo.png" style="margin-right: 8px;margin-left: 8px;"/></a>
    <a href="#main_page_content" style="text-align:center; font-size:20px;color:#7ca81e"> <i class="fa fa-home"></i></a>

<a href="#analysis_interpretability" id="sidebar_analysis"> <img height="25" src="../resources/lectures/ico/analysis_empty.png"/></a>
<div class="extra_components">
    <a href="#research_thinking" id="sidebar_research_thinking"><img height="30" src="../resources/lectures/ico/bulb_empty.png"/></a>
    <!--<hr color="#b7db67">-->

    <a href="#related_papers" id="sidebar_related_papers"><img height="22" src="../resources/lectures/ico/book_empty.png"/></a>
    <!--<hr color="#b7db67">-->

  <a href="#have_fun" id="sidebar_fun"><img height="30" src="../resources/lectures/ico/fun_empty.png" /></a>
</div>
</div>


<script>
function openNav() {
  document.getElementById("sidebar").style.display = "block";
  document.getElementById("sidebar_small").style.display = "none";
  document.getElementById("close_sidebar_btn").style.display = "block";
}

function closeNav() {
  document.getElementById("sidebar").style.display = "none";
  document.getElementById("sidebar_small").style.display = "block";
  document.getElementById("close_sidebar_btn").style.display = "none";
}

</script>


<script>
function onResize() {
  if (window.innerWidth >= 800) {
     document.getElementById("sidebar").style.display = "block";
     document.getElementById("sidebar_small").style.display = "none";
     document.getElementById("close_sidebar_btn").style.display = "none";
  }
  else {
     document.getElementById("sidebar").style.display = "none";
    document.getElementById("sidebar_small").style.display = "block";

  }
}
window.onresize = onResize;
onResize();
</script>

<script>
/* Loop through all dropdown buttons to toggle between hiding and showing its dropdown content - This allows the user to have multiple dropdowns without any conflict */
var dropdown = document.getElementsByClassName("dropdown-btn");
var i;

for (i = 0; i < dropdown.length; i++) {
  dropdown[i].addEventListener("click", function(event) {
  this.classList.toggle("active_caret");
  var dropdownButton = event.target || event.srcElement;
  while(dropdownButton.className != "dropdown-scope")
     dropdownButton = dropdownButton.parentElement;
  var dropdownContent = dropdownButton.getElementsByClassName("dropdown-container")[0];

  if (dropdownContent.style.display === "block") {
  dropdownContent.style.display = "none";
  } else {
  dropdownContent.style.display = "block";
  }
  });
}
</script>



<div class="wrapper" id="main_page_content">
    <div class="header"><h1>Text Classification</h1></div>

<div class="main_content" id="main_content">

<div id="intro">


     <div class="carousel" data-flickity='{ "imagesLoaded": true, "percentPosition": true, "wrapAround": true }'
     style="width:50%; margin-bottom:30px; margin-left:10px; float:right;">
              <div class="carousel-cell" style="width:100%"><center>
                    <img width=70% src="../resources/lectures/text_clf/intro/example_movie-min.png"/></center>
                  <p style="text-align:center;margin-left:30px;margin-right:30px;">
                      Multi-class classification:<br> many labels, only one correct</p>
              </div>
              <div class="carousel-cell" style="width:100%"><center>
                    <img width=70% src="../resources/lectures/text_clf/intro/example_mail-min.png"/></center>
                  <p style="text-align:center;margin-left:30px;margin-right:30px;">
                      Binary classification:<br> two labels, only one correct</p>
              </div>
              <div class="carousel-cell" style="width:100%"><center>
                    <img width=70% src="../resources/lectures/text_clf/intro/example_twitter-min.png"/></center>
                  <p style="text-align:center;margin-left:30px;margin-right:30px;">
                      Multi-label classification:<br> many labels, several can be correct</p>
              </div>
              <div class="carousel-cell" style="width:100%"><center>
                    <img width=70% src="../resources/lectures/text_clf/intro/example_document-min.png"/></center>
                  <p style="text-align:center;margin-left:30px;margin-right:30px;">
                      Multi-class classification:<br> many labels, only one correct</p>
              </div>
      </div>

    <p>Text classification is an extremely popular task.
    You enjoy working text classifiers in your mail agent:
    it classifies letters and filters spam. Other applications include
    document classification, review classification, etc.</p>

    <p>Text classifiers are often used not as an individual task, but as part of bigger pipelines.
    For example, a voice assistant classifies your utterance to understand what you want (e.g., set the alarm,
        order a taxi or just chat) and passes your message to different models depending
    on the classifier's decision. Another example is a web search engine: it
        can use classifiers to identify the query language, to predict
        the type of your query (e.g., informational, navigational, transactional),
    to understand whether you what to see pictures or video in addition to documents, etc. </p>

    <p>Since most of the classification datasets assume that only one label is correct (you will see this right now!),
        in the lecture we deal with this type of classification, i.e. the <font face="arial">single-label
        classification</font>.
        We mention multi-label classification in a separate section
        (<a href="#multi_label">Multi-Label Classification</a>).
    </p>




    <div id="dataset_examples">
    <h2>Datasets for Classification</h2>


        <p>Datasets for text classification are very different in
            terms of size (both dataset size and examples' size),
            what is classified, and the number of labels. Look at the statistics below.
        </p>

        <p>
<center>
<table class="data_text text_table" style="padding:5px;margin-bottom:0px;">
  <tr>
    <th style="text-align:left;">Dataset</th>
    <th>Type</th>
      <th>Number</br> of labels</th>

      <th> Size </br>(train/test)</th>

      <th>Avg. length</br> (tokens)</th>
  </tr>



  <tr>
    <td style="text-align:left;"><a href="https://nlp.stanford.edu/sentiment/index.html" target="_blank">SST</a></td>
    <td>sentiment</td>
      <td>5 or 2</td>
      <td>8.5k / 1.1k</td>

      <td>19</td>
  </tr>

  <tr>
    <td style="text-align:left;"><a href="https://ai.stanford.edu/~amaas/data/sentiment/" target="_blank">IMDb Review</a></td>
    <td>sentiment</td>
    <td>2</td>
      <td>25k / 25k</td>
      <td>271</td>
  </tr>

  <tr>
    <td style="text-align:left;"><a href="https://www.kaggle.com/yelp-dataset/yelp-dataset" target="_blank">Yelp Review</font></td>
    <td>sentiment</td>
    <td>5 or 2</td>
      <td>650k / 50k</td>
      <td>179</td>
  </tr>

   <tr>
    <td style="text-align:left;"><a href="https://www.kaggle.com/bittlingmayer/amazonreviews" target="_blank">Amazon Review</font></td>
    <td>sentiment</td>
    <td>5 or 2</td>
      <td>3m / 650k</td>
      <td>79</td>
  </tr>

   <tr>
    <td style="text-align:left;"><a href="https://cogcomp.seas.upenn.edu/Data/QA/QC/" target="_blank">TREC</a></td>

      <td>question</td>
    <td>6</td>
      <td>5.5k / 0.5k</td>
      <td>10</td>
  </tr>

    <tr>
        <td style="text-align:left;"><a href="https://www.kaggle.com/soumikrakshit/yahoo-answers-dataset" target="_blank">Yahoo! Answers</a></td>
    <td>question</td>
    <td>10</td>
      <td>1.4m / 60k</td>
      <td>131</td>
  </tr>

   <tr>
      <td style="text-align:left;">
        <a href="http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html" target="_blank">AG’s News</a></td>
      <td>topic</td>
      <td>4</td>
      <td>120k / 7.6k</td>
      <td>44</td>
  </tr>


   <tr>
      <td style="text-align:left;"><a href="http://www.sogou.com/labs/resource/cs.php" target="_blank">Sogou News</a></td>
      <td>topic</td>
      <td>6</td>
      <td>54k / 6k</td>
      <td>737</td>
  </tr>


   <tr>
      <td style="text-align:left;"><a href="https://wiki.dbpedia.org/services-resources/dbpedia-data-set-2014" target="_blank">DBPedia</a></td>
      <td>topic</td>
      <td>14</td>
      <td>560k / 70k</td>
      <td>67</td>
  </tr>


</table>

        <span class="data_text" style="text-align:center;font-size:15px;">
            Some of the datasets can be downloaded <a href="https://course.fast.ai/datasets#nlp" target="_blank">here</a>.</span>
</center>

        </p>

        <p>
            The most popular datasets are
            for <font face="arial">sentiment classification</font>. They consist of reviews of
            movies, places or restaurants, and products. There are also datasets for question type classification and
            topic classification.
        </p>

        <p>To better understand typical classification tasks,
            below you can look at the examples from different datasets.</p>

    <br>
    <img height="20" src="../resources/lectures/ico/paw_empty.png" style="float:left; margin-top:-10px;"/>
    <div class="box_green_left">

        <div class="text_box_green">
          <p class="data_text"><u>How to:</u> pick a dataset and look at the examples to get a feeling of the task.
          Or you can come back to this later!</p>
        </div>
<br>
        <fieldset class="data_text" style="border: 1px solid #d8e8b5;border-radius:5px;margin-left:20px;">
            <legend><font color="#79a123"><strong>Pick a dataset</strong></font></legend>

              <input type="radio" id="dataset_example_sst" name="dataset_example" value="sst" checked  onclick="pickDatasetExample()">
              <label for="dataset_example_sst"  style="margin-right:15px;">SST</label>

              <input type="radio" id="dataset_example_imdb" name="dataset_example" value="imdb" onclick="pickDatasetExample()">
              <label for="dataset_example_imdb"  style="margin-right:15px;">IMDb Review</label>

              <input type="radio" id="dataset_example_yelp" name="dataset_example" value="yelp" onclick="pickDatasetExample()">
              <label for="dataset_example_yelp"  style="margin-right:15px;">Yelp Review</label>

             <input type="radio" id="dataset_example_amazon" name="dataset_example" value="yelp" onclick="pickDatasetExample()">
              <label for="dataset_example_amazon"  style="margin-right:15px;">Amazon Review</label>

            <br>

              <input type="radio" id="dataset_example_trec" name="dataset_example" value="trec" onclick="pickDatasetExample()">
              <label for="dataset_example_trec" style="margin-right:15px;">TREC</label>

              <input type="radio" id="dataset_example_yahoo" name="dataset_example" value="yahoo" onclick="pickDatasetExample()">
              <label for="dataset_example_yahoo" style="margin-right:15px;">Yahoo! Answers</label>

              <input type="radio" id="dataset_example_ag_news" name="dataset_example" value="ag_news" onclick="pickDatasetExample()">
              <label for="dataset_example_ag_news"  style="margin-right:15px;">AG's News</label>

              <input type="radio" id="dataset_example_sogou_news" name="dataset_example" value="sogou_news" onclick="pickDatasetExample()">
              <label for="dataset_example_sogou_news"  style="margin-right:15px;">Sogou News</label>

              <input type="radio" id="dataset_example_dbpedia" name="dataset_example" value="dbpedia" onclick="pickDatasetExample()">
              <label for="dataset_example_dbpedia"  style="margin-right:15px;">DBPedia</label>
        </fieldset>

<br>



        <div id="dataset_info_sst" style="margin-left:20px;">
            <details>
                <summary><font color="#79a123" class="data_text"><strong>Dataset Description (click!)</strong></font></summary>
                <div  class="data_text" style="font-size:15px;">
                    <br>
            <p>SST is a sentiment classification dataset which consists of movie reviews
                (from Rotten Tomatoes html files). The dataset consists of parse trees of the sentences,
                and not only entire sentences, but also smaller phrases have a sentiment label.</p>


                <p>There are five labels: 1 (very negative), 2 (negative), 3 (neutral), 4 (positive),
                and 5 (very positive) (alternatively, labels can be 0-4). Depending on the used labels, you can get either
                    binary <strong>SST-2</strong> dataset (if you only consider positivity
                    and negativity) or fine-grained sentiment <strong>SST-5</strong> (when using all labels).</p>

            <p>Note that the dataset size mentioned above (8.5k/2.2k/1.1k for train/dev/test) is in the number of
            sentences. However, it also has 215,154 phrases that compose each sentence in the dataset.
            </p>
            <p>For more details, see <a href="https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf" target="_blank">
            the original paper</a>.</p>
            </div>
            </details>
            <p>Look how sentiment of a sentence is composed from its parts.</p>
        </div>
        <div class="carousel" id="carousel_sst_examples"
     data-flickity='{ "imagesLoaded": true, "percentPosition": true, "wrapAround": true}'
     style="width:100%; margin-top:10px; margin-bottom:30px; margin-left:10px;">

            <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: 3<br/>

                     <u>Review</u>:<br/>
                     Makes even the claustrophobic on-board quarters seem fun .
                     </p>
             </div>
              <img height=250 src="../resources/lectures/text_clf/intro/sst_tree_claustrophobic-min.png"
         style="" class="center"/>
         </center>
         </div>


           <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: 1<br/>

                     <u>Review</u>:<br/>
                     Ultimately feels empty and unsatisfying , like swallowing a Communion wafer without the wine .
                     </p>
             </div>
              <img height=250 src="../resources/lectures/text_clf/intro/sst_tree_communion_wafer-min.png"
         style="" class="center"/>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: 5<br/>

                     <u>Review</u>:<br/>
                     A quiet treasure -- a film to be savored .
                     </p>
             </div>
              <img height=250 src="../resources/lectures/text_clf/intro/sst_tree_treasure_savored-min.png"
         style="" class="center"/>
         </center>
         </div>







</div>

        <div id="dataset_info_imdb" style="margin-left:20px;">
            <details>
                <summary><font color="#79a123" class="data_text"><strong>Dataset Description (click!)</strong></font></summary>
                <div  class="data_text" style="font-size:15px;">
                    <br>
            <p>IMDb is a large dataset of informal movie reviews from the Internet Movie Database.
                The collection allows no more than 30 reviews per movie. The dataset contains an even number
                of positive and negative reviews, so randomly guessing yields 50% accuracy.
                The reviews are highly polarized: they are only negative (with the highest score 4 out of 10)
                or positive (with the lowest score 7 out of 10).
            </p>
                <p>For more details, see <a href="https://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf" target="_blank">
                    the original paper</a>.</p>
            </div>
            </details>
        </div>
        <div class="carousel" id="carousel_imdb_examples"
     data-flickity='{ "imagesLoaded": true, "percentPosition": true, "wrapAround": true}'
     style="width:100%; margin-top:10px; margin-bottom:30px; margin-left:10px;">

          <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: negative<br/><br/>

                     <u>Review</u><br/>
                    Hobgoblins .... Hobgoblins .... where do I begin?!?<br /><br />This film gives Manos -
                     The Hands of Fate and Future War a run for their money as the worst film ever made .
                     This one is fun to laugh at , where as Manos was just painful to watch . Hobgoblins will
                     end up in a time capsule somewhere as the perfect movie to describe the term : " 80 's cheeze " .
                     The acting ( and I am using this term loosely ) is atrocious , the Hobgoblins are some of the worst
                     puppets you will ever see , and the garden tool fight has to be seen to be believed .
                     The movie was the perfect vehicle for MST3 K , and that version is the only way to watch this mess .
                     This movie gives Mike and the bots lots of ammunition to pull some of the funniest one -
                     liners they have ever done . If you try to watch this without the help of Mike and the bots .....
                     God help you ! ! </p>
             </div>
         </center>
         </div>


          <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: positive<br/><br/>

                     <u>Review</u><br/>
                    One of my favorite movies I saw at preview in Seattle . Tom Hulce was amazing , with out words could
                     convey his feelings / thoughts . I actually sent Mike Ferrell some donation money to help the film
                     get distributed . It is good . System says I need more lines but do not want to give away plot stuff .
                     I was in the audience in Seattle with Hulce and director , a writer I think and Mike Ferrell .
                     They talked for about an hour afterwords . Not really a dry eye in the house . Why Hollywood continues
                     to be stupid I do not know . ( actually I do know , it is our fault , look what we watch)Well you get
                     what you pay for guys . Get this and see it with someone special . It is a gem . </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: negative<br/><br/>

                     <u>Review</u><br/>
                   Okay , if you have a couple hours to waste , or if you just really hate your life , I would say watch
                     this movie . If anything it 's good for a few laughs . Not only do you have obese , topless natives ,
                     but also special effects so bad they are probably outlawed in most states . Seriuosly ,
                     the rating of ' PG ' is pretty humorous too , once you see the Native Porn Extravaganza .
                     I would n't give this movie to my retarded nephew . You could n't even show this to Iraqi
                     prisoners without violating the Geneva Convention . The plot is sketchy , and cliché , and dumb ,
                     and stupid . The acting is horrible , and the ending is so painful to watch I actually began pouring
                     salt into my eye just to take my mind off of the idiocy filling my TV screen . </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: positive<br/><br/>

                     <u>Review</u><br/>
                   I really liked this movie ... it was cute . I enjoyed it , but if you did n't , that is your fault .
                     Emma Roberts played a good Nancy Drew , even though she is n't quite like the books . The old fashion
                     outfits are weird when you see them in modern times , but she looks good on them . To me , the rich
                     girls did n't have outfits that made them look rich . I mean , it looks like they got all the clothes
                     -blindfolded- at a garage sale and just decided to put it on all together . All of the outfits
                     were tacky , especially when they wore the penny loafers with their regular outfits . I do not
                     want to make the movie look bad , because it definitely was n't ! Just go to the theater and
                     watch it ! ! ! You will enjoy it ! </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: negative<br/><br/>

                     <u>Review</u><br/>
                   I always found Betsy Drake rather creepy , and this movie reinforces that . As another review said ,
                     this is a stalker movie that is n't very funny . I watched it because it has CG in it ,
                     but he hardly gets any screen time . It 's no " North by Northwest " ... </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: negative<br/><br/>

                     <u>Review</u><br/>
                   This movie was on t.v the other day , and I did n't enjoy it at all . The first George of the
                     jungle was a good comedy , but the sequel .... completely awful . The new actor and
                     actress to play the lead roles were n't good at all , they should of had the original
                     actor ( Brendon Fraiser ) and original actress ( i forgot her name ) so this movie gets
                     the 0 out of ten rating , not a film that you can sit down and watch and enjoy ,
                     this is a film that you turn to another channel or take it back to the shop if hired or
                     bought . It was good to see Ape the ape back , but was n't as fun as the first ,
                     they should of had the new George as Georges son grown up , and still had Bredon and
                     ( what s her face ) in the film , that would 've been a bit better then it was .</p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: positive<br/><br/>

                     <u>Review</u><br/>
                   I loved This Movie . When I saw it on Febuary 3rd I knew I had to buy It ! ! ! It comes out to buy
                     on July 24th ! ! ! It has cool deaths scenes , Hot girls , great cast , good story , good
                     acting . Great Slasher Film . the Movies is about some serial killer killing off four girls .
                     SEE this movies</p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: positive<br/><br/>

                     <u>Review</u><br/>
                   gone in 60 seconds is a very good action comedy film that made over $ 100 million but got blasted
                     by most critics . I personally thought this was a great film . The story was believable and
                     has probobly the greatest cast ever for this type of movie including 3 academy award winners
                     nicolas cage , robert duvall and the very hot anjolina jolie . other than the lame stunt
                     at the end this is a perfect blend of action comedy and drama .
                     my score is * * * * ( out of * * * * )</p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: positive<br/><br/>

                     <u>Review</u><br/>
                   This is one of the most interesting movies I have ever seen . I love the backwoods feel of this movie .
                     The movie is very realistic and believable . This seems to take place in another era , maybe
                     the late 60 's or early 70 's . Henry Thomas works well with the young baby . Very moving story
                     and worth a look .</p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: positive<br/><br/>

                     <u>Review</u><br/>
                   I admit it 's very silly , but I 've practically memorized the damn thing ! It holds a lot of good
                     childhood memories for me ( my brother and I saw it opening day ) and I have respect for
                     any movie with FNM on the soundtrack .</p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: positive<br/><br/>

                     <u>Review</u><br/>
                   I love the series ! Many of the stereotypes portraying Southerrners as hicks are very apparent ,
                     but such people do exist all too frequently . The portrayal of Southern government rings
                     all too true as well , but the sympathetic characters reminds one of the many good things
                     about the South as well . Some things never change , and we see the " good old boys " every day !
                     There is a Lucas Buck in every Southern town who has only to make a phone call to make things
                     happen , and the storybook " po ' white trash " are all too familiar . Aside from the supernatural
                     elements , everything else could very well happen in the modern South ! I somehow think Trinity ,
                     SC must have been in Barnwell County !</p>
             </div>
         </center>
         </div>

</div>

        <div id="dataset_info_yelp" style="margin-left:20px;">
            <details>
                <summary><font color="#79a123" class="data_text"><strong>Dataset Description (click!)</strong></font></summary>
                <div  class="data_text" style="font-size:15px;">
                    <br>
            <p>The Yelp reviews dataset is obtained from the
                <a href="https://www.kaggle.com/yelp-dataset/yelp-dataset" target="_blank">Yelp Dataset Challenge in 2015</a>.
                Depending on the number of labels, you can get either
                <strong>Yelp Full</strong> (all 5 labels) or
                <strong>Yelp Polarity</strong> (positive and negative classes) dataset.
                <strong>Full</strong>
                has 130,000 training samples and 10,000
                testing samples in each star, and the <strong>Polarity</strong> dataset has 280,000
                training samples and 19,000 test samples in each polarity.
            </p>
                <p>For more details, see <a href="https://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf" target="_blank">
                    the Kaggle Challenge page</a>.</p>
            </div>
            </details>
        </div>
        <div class="carousel" id="carousel_yelp_examples"
     data-flickity='{ "imagesLoaded": true, "percentPosition": true, "wrapAround": true}'
     style="width:100%; margin-top:10px; margin-bottom:30px; margin-left:10px;">

          <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: 4<br/><br/>

                     <u>Review</u><br/>
                   I had a serious craving for Roti.  So glad I found this place.
                     A very small menu selection but it had exactly what I wanted.  The serving for
                     $8.20 after tax is enough for 2 meals.  I know where to go from now on for a great
                     meal with leftovers.  This is a noteworthy place to bring my Uncle T.J. who's a Trini
                     when he comes to visit.</p>
             </div>
         </center>
         </div>


          <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: 2<br/><br/>

                     <u>Review</u><br/>
                   The actual restaurant is fine, the service is friendly and good.<br/>

I am not going to go in to the food other than to say, no.<br/>

Oh well $340 bucks and all I can muster is a no. </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: 5<br/><br/>

                     <u>Review</u><br/>
                   What a cool little place tucked away behind a strip mall.  Would never have found
                     this if it was not suggested by a good friend who raved about the cappuccino!
                     He is world traveler, so, it's a  must try if it's the best cup he's ever had.
                     He was right!  Don't know if it's in the beans or the care that they take to make
                     it with a fab froth decoration on top.  My hubby and I loved the caramel brulee taste..
                     My son loved the hot ""warm"" cocoa.  Yeah, we walked in as a family last night and
                     almost everyone turned our way since we did not fit the hip college crowd.

                     Everyone was really friendly, though. The sweet young man behind the
                     counter gave my son some micro cinnamon doughnuts and scored major points with the
                     little dude!   We will be back. </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: 3<br/><br/>

                     <u>Review</u><br/>
                   Jersey Mike's is okay. It's a chain place, and a bit over priced for fast food.
                     I ordered a philly cheese steak.  It was mostly bread, with a few
                     thing microscopic slices of meat.  A little cheese too.  And a sliver or
                     two of peppers.  But mostly, it was bread.  I think it's funny the people
                     that work here try to make small talk with you.  "So, what are you guys up to tonight?"
                     I think it would be fun to just try and f*#k with them, and say something like,
                     "Oh you know, smoking a little meth and just chilling with some hookers."
                     See what they say to that. </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: 5<br/><br/>

                     <u>Review</u><br/>
                   Love it!!! Wish we still lived in Arizona as Chino is the one thing we miss.
                     Every time I think about Chino Bandido my mouth starts watering.
                     If I am ever in the state again I will drive out of my way just to go to it again. YUMMY! </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: 4<br/><br/>

                     <u>Review</u><br/>
                  I have been here a few times, but mainly at dinner.  Dinner Has always been great, great
                     waiters and staff, with a LCD TV playing almost famous famous bolywood movies.

                     LOL  It cracks me up when they dance.....LOL...anyhow, Great vegetarian choices for
                     people who eat there veggies, but trust me, I am a MEAT eater,  Chicekn Tika masala,
                     Little dry but still good.  My favorite is there Palak Paneer. Great for the vegetarian.
                     I have also tried there lunch Buffet for 11.00.  I give a Thumbs up..  Good,
                     serve yourself, and pig out!!!!!!</p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: 3<br/><br/>

                     <u>Review</u><br/>
                   Very VERY average. Actually disappointed in the braciole. Kelsey with the pretty smile and form
                     fitting shorts would probably make me think about going back and trying something different.</p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: 1<br/><br/>

                     <u>Review</u><br/>
                   So, what kind of place serves chicken fingers and NO Ranch dressing?????? The only sauces they
                     had was honey mustard and ""Canes Secret sauce"" Can I say EEWWWW!! I thought that
                     the sauce tasted terrible. I am not too big a fan of honey mustard but I do On occasion
                     eat it if there is nothing else And that wasn't even good! The coleslaw was awful also.
                     I do have to say that the chicken fingers were very juicy but also very bland.
                     Those were the only 2 items that I tried, not that there were really any more items  on
                     the menu, Texas toast? And Fries I think?? Overall, I would never go back.</p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: 4<br/><br/>

                     <u>Review</u><br/>
                   Good food, good drinks, fun bar.
There are quite a few Buffalo Wild Wings in the valley and they're a fun place to grab a quick lunch or dinner and watch the game.
They have a pretty good garden burger and their buffalo chips (french fry-ish things) are really good.
If you like bloody mary's, they have the best one. It's so good...really spicy and filled with celery and olives.
Be careful when you come though, if there is a game on, you'll have to get there early or you definitely won't get a spot to sit.</p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: 5<br/><br/>

                     <u>Review</u><br/>
                   Excellent in every way.  Attentive and fun owners who tend bar every weekend night - GREAT live music,
                     excellent wine selection.   Keeping Fountain Hills young.... one weekend at a time.</p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: 1<br/><br/>

                     <u>Review</u><br/>
                   After repeat visits it just gets worse - the service, that is.  It was as if we were held
                     hostage and could not leave for a full 25 minutes because that's how long it took to receive
                     hour check after several requests to several different employees.  Female servers might
                     be somewhat cute but know absolutely nothing about beer and this is a brewpub.
                     I asked if they have an seasonal beers and the reply was no, that they only sell
                     their own beers!  Even more amusing is their ""industrial pale ale"" is an IPA but it
                     is not bitter.  So, they say it's an IPA but it's not bitter, it's not a true-to-style IPA.
                     Then people say ""Oh I don't like IPA's"" and want something else.  Their attempt
                     to rename a beer/style is actually hurting them.  Amazing.</p>
             </div>
         </center>
         </div>

</div>

        <div id="dataset_info_amazon" style="margin-left:20px;">
            <details>
                <summary><font color="#79a123" class="data_text"><strong>Dataset Description (click!)</strong></font></summary>
                <div  class="data_text" style="font-size:15px;">
                    <br>
            <p>The Amazon reviews dataset consists of reviews from amazon
                which include product and user information, ratings, and a plaintext review.
                The dataset is obtained from the <a href="https://snap.stanford.edu/data/web-Amazon.html" target="_blank">
                     Stanford Network Analysis Project (SNAP)</a>.
                Depending on the number of labels, you can get either
                <strong>Amazon Full</strong> (all 5 labels) or
                <strong>Amazon Polarity</strong> (positive and negative classes) dataset.
                <strong>Full</strong>
                has 600,000 training samples and 130,000
                testing samples in each star, and the <strong>Polarity</strong> dataset has 1800,000
                training samples and 200,000 test samples in each polarity.
                The fields used are review title and review content.
            </p>

            </div>
            </details>
        </div>
        <div class="carousel" id="carousel_amazon_examples"
     data-flickity='{ "imagesLoaded": true, "percentPosition": true, "wrapAround": true}'
     style="width:100%; margin-top:10px; margin-bottom:30px; margin-left:10px;">

          <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: 4<br/><br/>

                     <u>Review Title</u>: good for a young reader<br/><br/>

                     <u>Review Content</u>:<br/>
                     just got this book since i read it when i was younger and loved it. i will reread it one
                     of these days, but i bet its pretty lame 15 years later, oh well.

                 </p>
             </div>
         </center>
         </div>


          <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: 2<br/><br/>

                     <u>Review Title</u>: The Castle in the Attic<br/><br/>

                     <u>Review Content</u>:<br/>
                     i read the castle in the attic and i thoght it wasn't very entrtaning.
                     but that's just my opinion. i thought on some of the chapters it dragged
                     on and lost me alot. like it would talk about one thing and then another without
                     alot of detail. for my opinion, it was an ok book. it had it's moments like whats
                     going to happen next then it was really boring.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: 1<br/><br/>

                     <u>Review Title</u>: worst book in the world!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!<br/><br/>

                     <u>Review Content</u>:<br/>
                     This was the worst book I have ever read in my entire life! I was forced to read it for
                     school. It was complicated and very boring. I would not recommend this book to anyone!!
                     Please don't waste your time and money to read this book!! Read something else!!
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: 3<br/><br/>

                     <u>Review Title</u>: It's okay.<br/><br/>

                     <u>Review Content</u>:<br/>
                     I'm using it for track at school as a sprinter. It's okay, but to hold it together
                     is a velcro strap. So you either use the laces or take them out and use only the velcro.
                     Also, if you order them, order them a half size or a full size biggerthen your normal
                     size or else it'll be a tight squeeze. Plus side, you can still run on your toes in them.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: 5<br/><br/>

                     <u>Review Title</u>: Time Well Spent<br/><br/>

                     <u>Review Content</u>:<br/>
                     For those beginning to read the classics this one is a great hook. While the characters are
                     complex the story is linear and the allusions are simple enough to follow. One can't help
                     but hope Tess's life will somehow turn out right although knowing it will not. The burdens
                     she encounters seem to do little to stop her from moving forward. Life seems so unfair
                     to her, but Hardy handles her masterfully; indeed it is safe to say Hardy loves her more
                     than God does.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: 2<br/><br/>

                     <u>Review Title</u>: Not a brilliant book but...<br/><br/>

                     <u>Review Content</u>:<br/>
                     I didn't like this book very much. It was silly. I'm sorry that I've wasted time reading
                     this book. There are better books to read.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: 3<br/><br/>

                     <u>Review Title</u>: Simple<br/><br/>

                     <u>Review Content</u>:<br/>
                     This book was not anything special. Although I love romances, it was too simple.
                     The symbolism was spelled out to the readers in a blunt manner. The less educated
                     readers may appreciate it. The wording was quite beautiful at times and the plot was
                     enchanting (perfect for a movie) but it is not heart wrenching like the movie Titantic
                     (which was a must see!) ;)
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: 1<br/><br/>

                     <u>Review Title</u>: WHY???????????????????????????????????????????????????<br/><br/>

                     <u>Review Content</u>:<br/>
                     WHY are poor, innocent school kids forced to read this intensly dull text? It is
                     enough to put anyone off English Lit for LIFE. If anyone in authority is reading
                     this, PLEASE take this piece of junk OFF the sylabus...PLEASE!!!!!!!!!!!
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: 4<br/><br/>

                     <u>Review Title</u>: looking back<br/><br/>

                     <u>Review Content</u>:<br/>
                     I have read several Thomas Hardy novels starting with The Mayor of Casterbridge many years
                     ago in high school and I never really appreciated the style and the fact that like other Hardy
                     novels Tess is a love story and a very good story. Worth reading
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: 5<br/><br/>

                     <u>Review Title</u>: Great Puzzle<br/><br/>

                     <u>Review Content</u>:<br/>
                     This is an excellent puzzle for very young children. Melissa & Doug products are well made and kids
                     love them. This puzzle is wooden so kids wont destroy it if they try to roughly put the pieces in.
                     The design is adorable and makes a great gift for any young animal lover.
                 </p>
             </div>
         </center>
         </div>

</div>

        <div id="dataset_info_trec" style="margin-left:20px;">
            <details>
                <summary><font color="#79a123" class="data_text"><strong>Dataset Description (click!)</strong></font></summary>
                <div  class="data_text" style="font-size:15px;">
                    <br>
            <p>TREC is a dataset for classification of free factual questions.
                It defines a two-layered taxonomy, which represents a natural semantic classification for
                typical answers in the
                TREC task. The hierarchy contains 6 coarse classes (ABBREVIATION, ENTITY,
                DESCRIPTION, HUMAN, LOCATION and NUMERIC VALUE) and 50 fine classes.
            </p>
                <p>For more details, see <a href="https://www.aclweb.org/anthology/C02-1150.pdf" target="_blank">
                    the original paper</a>.</p>
            </div>
            </details>
        </div>
        <div class="carousel" id="carousel_trec_examples"
     data-flickity='{ "imagesLoaded": true, "percentPosition": true, "wrapAround": true}'
     style="width:100%; margin-top:10px; margin-bottom:30px; margin-left:10px;">

          <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">
                     <u>Label</u>: DESC (description)<br/>

                     <u>Question</u>: How did serfdom develop in and then leave Russia ?<br/><br/>

                      <u>Label</u>: ENTY (entity)<br/>

                     <u>Question</u>: What films featured the character Popeye Doyle ?<br/><br/>

                     <u>Label</u>: HUM (human)<br/>

                     <u>Question</u>: What team did baseball 's St. Louis Browns become ?<br/><br/>

                     <u>Label</u>: HUM (human)<br/>

                     <u>Question</u>: What is the oldest profession ?

                     </p>
             </div>
         </center>
         </div>

            <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">

                     <u>Label</u>: DESC (description)<br/>

                     <u>Question</u>: How can I find a list of celebrities ' real names ?<br/><br/>

                     <u>Label</u>: ENTY (entity)<br/>

                     <u>Question</u>: What fowl grabs the spotlight after the Chinese Year of the Monkey ?<br/><br/>

                     <u>Label</u>: ABBR (abbreviation)<br/>

                     <u>Question</u>: What is the full form of .com ?<br/><br/>

                     <u>Label</u>: ENTY (entity)<br/>

                     <u>Question</u>: What 's the second - most - used vowel in English ?

                     </p>
             </div>
         </center>
         </div>


         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">

                     <u>Label</u>: DESC (description)<br/>

                     <u>Question</u>: What are liver enzymes ?<br/><br/>

                     <u>Label</u>: HUM (human)<br/>

                     <u>Question</u>: Name the scar-faced bounty hunter of The Old West .<br/><br/>

                     <u>Label</u>: NUM (numeric value)<br/>

                     <u>Question</u>: When was Ozzy Osbourne born ?<br/><br/>

                     <u>Label</u>: DESC (description)<br/>

                     <u>Question</u>: Why do heavier objects travel downhill faster ?

                     </p>
             </div>
         </center>
         </div>


         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">

                     <u>Label</u>: HUM (human)<br/>

                     <u>Question</u>: Who was The Pride of the Yankees ?<br/><br/>

                     <u>Label</u>: HUM (human)<br/>

                     <u>Question</u>: Who killed Gandhi ?<br/><br/>

                     <u>Label</u>: LOC (location)<br/>

                     <u>Question</u>: What sprawling U.S. state boasts the most airports ?<br/><br/>

                     <u>Label</u>: DESC (description)<br/>

                     <u>Question</u>: What did the only repealed amendment to the U.S. Constitution deal with ?

                     </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">

                     <u>Label</u>: NUM (numeric value)<br/>

                     <u>Question</u>: How many Jews were executed in concentration camps during WWII ?<br/><br/>

                     <u>Label</u>: DESC (description)<br/>

                     <u>Question</u>: What is " Nine Inch Nails " ?<br/><br/>

                     <u>Label</u>: DESC (description)<br/>

                     <u>Question</u>: What is an annotated bibliography ?<br/><br/>

                     <u>Label</u>: NUM (numeric value)<br/>

                     <u>Question</u>: What is the date of Boxing Day ?

                     </p>
             </div>
         </center>
         </div>

          <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:15px;">

                     <u>Label</u>: ENTY (entity)<br/>

                     <u>Question</u>: What articles of clothing are tokens in Monopoly ?<br/><br/>

                     <u>Label</u>: HUM (human)<br/>

                     <u>Question</u>: Name 11 famous martyrs .<br/><br/>

                     <u>Label</u>: DESC (description)<br/>

                     <u>Question</u>: What 's the Olympic motto ?<br/><br/>

                     <u>Label</u>: NUM (numeric value)<br/>

                     <u>Question</u>: What is the origin of the name ` Scarlett ' ?

                     </p>
             </div>
         </center>
         </div>


</div>

        <div id="dataset_info_yahoo" style="margin-left:20px;">
            <details>
                <summary><font color="#79a123" class="data_text"><strong>Dataset Description (click!)</strong></font></summary>
                <div  class="data_text" style="font-size:15px;">
                    <br>
            <p>The dataset is gathered from
                <a href="https://webscope.sandbox.yahoo.com/catalog.php?datatype=l" target="_blank">
                    Yahoo! Answers Comprehensive Questions and Answers version 1.0 dataset</a>.
                In contains the 10 largest main categories:

                "Society & Culture",
"Science & Mathematics",
"Health,
"Education & Reference",
"Computers & Internet",
"Sports",
"Business & Finance",
"Entertainment & Music",
"Family & Relationships",
"Politics & Government".
                Each class contains 140,000 training samples and 5,000 testing samples.
                The data consists of question title and content, as well as the best answer.

            </p>

            </div>
            </details>
        </div>
        <div class="carousel" id="carousel_yahoo_examples"
     data-flickity='{ "imagesLoaded": true, "percentPosition": true, "wrapAround": true}'
     style="width:100%; margin-top:10px; margin-bottom:30px; margin-left:10px;">

          <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Society & Culture<br/><br/>

                     <u>Question Title</u>: Why do people have the bird, turkey for thanksgiving?<br/><br/>

                     <u>Question Content</u>: Why this bird? Any Significance?<br/><br/>

                     <u>Best Answer</u><br/>
                    It is believed that the pilgrims and indians shared wild turkey and venison on the
                     original Thanksgiving. <br/><br/>Turkey's "Americanness" was established by
                     Benjamin Franklin, who had advocated for the turkey, not the bald eagle,
                     becoming the national bird.
                 </p>
             </div>
         </center>
         </div>


          <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Science & Mathematics<br/><br/>

                     <u>Question Title</u>: What is an "imaginary number"?<br/><br/>

                     <u>Question Content</u>: What is an "imaginary number",
                     and how is it treated in algebra equations?<br/><br/>

                     <u>Best Answer</u><br/>
                    Imaginary numbers are numbers than when squared equal a negative number, as in i^2 = -1,
                     where i is the imaginary number. You'll also often see them represented as i = √-1
                     (that's the square root of -1).<br/>Don't be confused by the poorly chosen name -

                     imaginary numbers do indeed exist and are used in advanced math, such as in the
                     physics of electromagnetic fields. The analogy that Wikipedia uses is a good one -
                     just like you don't need the concept of fractions to count stones, it doesn't mean that
                     fractions don't exist. :)
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Health<br/><br/>

                     <u>Question Title</u>: Does echinacea really help prevent colds?<br/><br/>

                     <u>Question Content</u>: Or is a waste of money...<br/><br/>

                     <u>Best Answer</u><br/>
                    Well, there appears to be some controvery about this.  While some people swear by the
                     stuff, others say that it has no real effect on overcoming a cold.  <br/>Here are some
                     links, one of which is from a National Institute of Health study.  I hope these help
                     you decide whether to head to the health store or not.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Education & Reference<br/><br/>

                     <u>Question Title</u>: How do I find an out of print book?<br/><br/>

                     <u>Question Content</u>: When I was a kid I remember seeing a book that was like an
                     yearbook of all newspapers published by the Times during WW II.
                     Each of the years is compiled into a different book. It gave one a very
                     uniqie perspecitev into the UK druing the war, and even had advertisements
                     from thaat time. Anybody out there know how to track such books?<br/><br/>

                     <u>Best Answer</u><br/>
                    here are several websites that you can find rare or out of print books.
                     A couple would be alibris.com or abebooks.com.  These sites list books by
                     booksellers all over the country and some internationally.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Computers & Internet<br/><br/>

                     <u>Question Title</u>: How can I record audio directly from the browser to the web server?<br/><br/>

                     <u>Question Content</u>: For a podcasting application, I'd like my web server
                     to be able to receive audio straight from the browser. Something like a "Push to talk"
                     button. It seems it's possible to do this with Flash. Is there any other way?<br/><br/>With
                     Flash, do I need to buy a Macromedia server licence, or are there alternatives to have
                     Flash on the browser talk to my server?<br/><br/>

                     <u>Best Answer</u><br/>
                    Userplane has an audio/video recorder that will do that -
                     you can check it out at http://www.userplane.com/apps/videoRecorder.cfm
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                  <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Sports<br/><br/>

                     <u>Question Title</u>: Why doesn't the NBA implement a minor leagues?<br/><br/>

                     <u>Question Content</u>: I don't want to see any more High School kids on the court,
                      shooting airballs and missing defensive assignments.<br/><br/>

                     <u>Best Answer</u><br/>
                    The NBA does have minor leagues - they're called the CBA, and the International leagues. :)
                      <br/>Seriously - because viewers seem to value explosiveness over efficiency,
                      I think we're seeing a major shift in the average age of NBA players towards
                      young athletes that are quicker, high-flying and more resilient to injury.
                      I wouldn't be surprised at all if by the end of this decade the average age of the
                      league allstars is well under 25.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Business & Finance<br/><br/>

                     <u>Question Title</u>: When will Google buy Yahoo?<br/><br/>

                     <u>Question Content</u>: The two businesses are very complementary in terms
                     of strengths and weaknesses. Do we want to beat ourselves up competing with
                     each other for resources and market share, or unite to beat MSFT?<br/><br/>

                     <u>Best Answer</u><br/>
                    Their respective market caps are too close for this to ever happen.<br/>Interestingly,
                     many reporters, analysts and tech pundits that I talk to think that the supposed
                     competition between Google and Yahoo is fallacious, and that they are very different
                     companies with very different strategies. Google's true competitor is often seen as
                     being Microsoft, not Yahoo. This would support your claim that they are complementary.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Entertainment & Music<br/><br/>

                     <u>Question Title</u>: Can someone tell me what happened in Buffy's series finale?<br/><br/>

                     <u>Question Content</u>: I had to work and missed the ending.<br/><br/>

                     <u>Best Answer</u><br/>
                    The gang makes an attack on the First's army, aided by Willow, who performs a powerful
                    spell to imbue all of the Potentials with Slayer powers. Meanwhile, wearing the amulet
                    that Angel brought, Spike becomes the decisive factor in the victory, and Sunnydale is
                    eradicated. Buffy and the gang look back on what's left of Sunnydale, deciding what
                    to do next...<br/>--but more importantly, there will no longer be any slaying in Sunnydale,
                    or is that Sunnyvale....
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Family & Relationships<br/><br/>

                     <u>Question Title</u>: How do you know if you're in love?<br/><br/>

                     <u>Question Content</u>: Is it possible to know for sure?<br/><br/>

                     <u>Best Answer</u><br/>
                    In my experience you just know. It's a long term feeling of always wanting to share
                     each new experience with the other person in order to make them happy, to laugh or
                     to know what they think about it. It's jonesing to call even though you just got off
                     an hour long phone call with them. It's knowing that being with them makes you a
                     better person. It's all of the above and much more.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Politics & Government<br/><br/>

                     <u>Question Title</u>: How come it seems like Lottery winners are always the
                     ones that buy tickets in low income areas.?<br/><br/>

                     <u>Question Content</u>: Pure luck or Government's way of trying to balance the rich and the poor.<br/><br/>

                     <u>Best Answer</u><br/>
                    I would put it down to psychology. People who feel they are well-off feel no need to participate in the
                     lottery programs. While those who feel they are less than well off think "Why not bet a buck or two
                     on the chance to make a few million?". It would seem to make sense to me.<br/><br/>addition: Yes Matt -
                     agreed. I just didn't state it as eloquently. Feeling 'no need to participate' is as you say related to
                     education, and those well off tend to have a better education.
                 </p>
             </div>
         </center>
         </div>



</div>

        <div id="dataset_info_ag_news" style="margin-left:20px;">
            <details>
                <summary><font color="#79a123" class="data_text"><strong>Dataset Description (click!)</strong></font></summary>
                <div  class="data_text" style="font-size:15px;">
                    <br>
            <p>The AG’s corpus was obtained from
                <a href="http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html" target="_blank">news articles on the web</a>.
                From these articles,
                only the AG’s corpus contains only
                the title and description fields from the
                the 4 largest classes.

            </p>
                <p>The dataset was introduced in
                    <a href="https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf" target="_blank">
                    this paper</a>.</p>
            </div>
            </details>
        </div>
        <div class="carousel" id="carousel_ag_news_examples"
     data-flickity='{ "imagesLoaded": true, "percentPosition": true, "wrapAround": true}'
     style="width:100%; margin-top:10px; margin-bottom:30px; margin-left:10px;">

          <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Sci/Tech<br/><br/>

                     <u>Title</u>: Learning to write with classroom blogs<br/><br/>

                     <u>Description</u><br/>
                    Last spring Marisa Dudiak took her second-grade class in Frederick County,
                     Maryland, on a field trip to an American Indian farm. </p>
             </div>
         </center>
         </div>


          <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Sports<br/><br/>

                     <u>Title</u>: Schumacher Triumphs as Ferrari Seals Formula One Title<br/><br/>

                     <u>Description</u><br/>
                    BUDAPEST (Reuters) - Michael Schumacher cruised to a record  12th win of the season in the Hungarian
                     Grand Prix on Sunday to  hand his Ferrari team a sixth successive constructors' title.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Business<br/><br/>

                     <u>Title</u>: DoCoMo and Motorola talk phones<br/><br/>

                     <u>Description</u><br/>
                    Japanese mobile phone company DoCoMo is in talks to buy 3G handsets from Motorola,
                     the world's second largest handset maker.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: World<br/><br/>

                     <u>Title</u>: Sharon 'backs settlement homes'<br/><br/>

                     <u>Description</u><br/>
                    Reports say Israeli PM Ariel Sharon has given the green light to new homes in West Bank settlements.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Business<br/><br/>

                     <u>Title</u>: Why Hugo Chavez Won a Landslide Victory<br/><br/>

                     <u>Description</u><br/>
                    When the rule of Venezuelan President Hugo Chavez was reaffirmed in a landslide 58-42 percent
                     victory on Sunday, the opposition who put the recall vote on the ballot was stunned.
                     They obviously don't spend much time in the nation's poor neighborhoods.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                  <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Sci/Tech<br/><br/>

                     <u>Title</u>: Free-Speech for Online Gambling Ads Sought<br/><br/>

                     <u>Description</u><br/>
                    The operator of a gambling news site on the Internet has asked a federal judge to declare that
                      advertisements in U.S. media for foreign online casinos and sports betting outlets
                      are protected by free-speech rights.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: World<br/><br/>

                     <u>Title</u>: Kerry takes legal action against Vietnam critics (AFP)<br/><br/>

                     <u>Description</u><br/>
                    AFP - Democratic White House hopeful John Kerry's campaign formally alleged that a group attacking
                     his Vietnam war record had illegal ties to US President George W. Bush's reelection bid.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Sports<br/><br/>

                     <u>Title</u>: O'Leary: I won't quit<br/><br/>

                     <u>Description</u><br/>
                    The Villa manager was said to be ready to leave the midlands club unless his assistants
                    Roy Aitken and Steve McGregor were also given new three-and-a-half year deals.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: World<br/><br/>

                     <u>Title</u>: Egypt eyes possible return of ambassador to Israel<br/><br/>

                     <u>Description</u><br/>
                    CAIRO - Egypt raised the possibility Tuesday of returning an ambassador to Israel soon, according
                     to the official Mena news agency, a move that would signal a revival of full diplomatic ties
                     after a four-year break.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Sports<br/><br/>

                     <u>Title</u>: Henry wants silverware<br/><br/>

                     <u>Description</u><br/>
                    Arsenal striker Thierry Henry insisted there must be an end product to the Gunners'
                     record-breaking run. As Arsenal equalled Nottingham Forest's 42-game unbeaten League
                     run Henry said:  "Even on the pitch we didn't realise what we had done."
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Sci/Tech<br/><br/>

                     <u>Title</u>: Scientists Focus on Algae in Maine Lake (AP)<br/><br/>

                     <u>Description</u><br/>
                    AP - Scientists would kill possibly thousands of white perch under a project to help restore
                     the ecological balance of East Pond in the Belgrade chain of lakes in central Maine.
                 </p>
             </div>
         </center>
         </div>

</div>

        <div id="dataset_info_sogou_news" style="margin-left:20px;">
            <details>
                <summary><font color="#79a123" class="data_text"><strong>Dataset Description (click!)</strong></font></summary>
                <div  class="data_text" style="font-size:15px;">
                    <br>
            <p>The Sogou News corpus
                was obtained from the combination of the
                <a href="http://www.ra.ethz.ch/cdstore/www2008/www2008.org/papers/pdf/p457-wang.pdf" target="_blank">
                    SogouCA and SogouCS news corpora.
                </a>

                The dataset consists of news articles (title and content fields) labeled with 5 categories:
                “sports”, “finance”, “entertainment”, “automobile” and “technology”.
                <br><br>
                The original dataset is in Chinese, but you can produce Pinyin – a
                phonetic romanization of Chinese.
                You can do it using
                <a href="https://pypinyin.readthedocs.io/zh_CN/v0.9.1/" target="_blank"><font face="courier">pypinyin</font></a>
                package combined with <a href="https://pypi.org/project/jieba/" target="_blank"><font face="courier">jieba</font></a>
                Chinese segmentation system (this is what
                <a href="https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf" target="_blank">
                    the paper introducing the dataset</a> did, and this is what I show you in the examples).
                The models for English can then be applied to this dataset without change.
            </p>

                <p>The dataset was introduced in
                    <a href="https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf" target="_blank">
                    this paper</a>,
                    the dataset in Pinyin can be downloaded
                    <a href="https://course.fast.ai/datasets#nlp" target="_blank">here</a>.</p>

                    <p class="data_text"><font color="#888"><u>Lena</u>: Here I picked very small texts - usually, the
        samples are much longer.</font></p>
            </div>
            </details>
        </div>
        <div class="carousel" id="carousel_sogou_news_examples"
     data-flickity='{ "imagesLoaded": true, "percentPosition": true, "wrapAround": true}'
     style="width:100%; margin-top:10px; margin-bottom:30px; margin-left:10px;">

          <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: automobile<br/><br/>

                     <u>Title</u>: tu2 we2n -LG be1i be3n sa4i di4 2 lu2n zha4n ba4
                     cha2ng ha4o ko3ng jie2 de3ng qi2 sho3u fu4 pa2n ta3o lu4n<br/><br/>

                     <u>Content</u><br/>
                    xi1n la4ng ti3 yu4 xu4n   be3i ji1ng shi2 jia1n 5 yue4 28 ri4 ,LG be1i shi4 jie4 qi2 wa2ng sa4i
                     be3n sa4i di4 2 lu2n za4i ha2n guo2 ka1i zha4n . zho1ng guo2 qi2 sho3u cha2ng ha4o ,
                     gu3 li4 , wa2ng ya2o , shi2 yue4 ca1n jia1 bi3 sa4i .<br/>  tu2 we2i xia4n cha3ng shu4n jia1n .
                     <br/>  wa3ng ye4 <br/>  bu4 zhi1 chi2 Flash </p>
             </div>
         </center>
         </div>


          <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: automobile<br/><br/>

                     <u>Title</u>:  qi4 che1 pi2n da4o<br/><br/>

                     <u>Content</u><br/>
                    xi1n we2n jia3n suo3 :<br/>  ke4 la2i si1 le4 300C<br/>  go4ng 20  zha1ng <br/>
                     ke3 shi3 jia4n pa2n ca1o zuo4 [ fa1ng xia4ng jia4n l: sha4ng yi1 zha1ng ;
                     fa1ng xia4ng jia4n r: xia4 yi1 zha1ng ; hui2 che1 : zha1 ka4n yua2n da4 tu2 ]\<br/>
                     ge1ng duo1 tu2 pia4n :<br/>  ce4 hua4 : bia1n ji2 : me3i bia1n : zhi4 zuo4 :GOODA  ji4 shu4 :
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: finance<br/><br/>

                     <u>Title</u>: shi2 da2 qi1 huo4 : hua2ng ji1n za3o pi2ng (06-11)<br/><br/>

                     <u>Content</u><br/>
                    shi4 cha3ng jia1 da4 me3i guo2 she1ng xi1 yu4 qi1 , me3i yua2n ji4n qi1 zo3u shi4 ba3o
                     chi2 de2 xia1ng da1ng pi2ng we3n , ji1n jia4 ga1o we4i mi2ng xia3n sho4u ya1 ,
                     xia4 jia4ng to1ng da4o ba3o chi2 wa2n ha3o , zhe4n da4ng si1 lu4 ca1o zuo4 .<br/>
                     gua1n wa4ng <br/>  wa3ng ye4 <br/>  bu4 zhi1 chi2 Flash<br/>  hua2ng ji1n qi1 huo4 zi1 xu4n la2n mu4
                 </p>
             </div>
         </center>
         </div>
</div>

        <div id="dataset_info_dbpedia" style="margin-left:20px;">
            <details>
                <summary><font color="#79a123" class="data_text"><strong>Dataset Description (click!)</strong></font></summary>
                <div  class="data_text" style="font-size:15px;">
                    <br>
            <p>DBpedia is a crow-sourced community effort to extract structured
            information from Wikipedia.
                The DBpedia ontology classification dataset is constructed by picking 14 non-overlapping
                classes from
                <a href="https://wiki.dbpedia.org/services-resources/dbpedia-data-set-2014" target="_blank">DBpedia 2014</a>.
                From each of the 14 ontology classes,
                the dataset contains
                 40,000 randomly chosen training samples and 5,000 testing samples.
                Therefore, the total size of the training dataset is 560,000, the testing dataset - 70,000.
            </p>

                <p>The dataset was introduced in
                    <a href="https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf" target="_blank">
                    this paper</a>.</p>
            </div>
            </details>
        </div>
        <div class="carousel" id="carousel_dbpedia_examples"
     data-flickity='{ "imagesLoaded": true, "percentPosition": true, "wrapAround": true}'
     style="width:100%; margin-top:10px; margin-bottom:30px; margin-left:10px;">

          <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Company<br/><br/>

                     <u>Title</u>: Marvell Software Solutions Israel<br/><br/>

                     <u>Abstract</u><br/>
                    Marvell Software Solutions Israel known as RADLAN Computer Communications
                     Limited before 2007 is a wholly owned subsidiary of Marvell Technology Group that
                     specializes in local area network (LAN) technologies. </p>
             </div>
         </center>
         </div>


          <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: EducationalInstitution<br/><br/>

                     <u>Title</u>:  Adarsh English Boarding School<br/><br/>

                     <u>Abstract</u><br/>
                    Adarsh English Boarding School is coeducational boarding school in Phulbari a suburb
                     of Pokhara Nepal. Nabaraj Thapa is the founder and chairman of the school. The School
                     motto reads Education For Better Citizen.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Artist<br/><br/>

                     <u>Title</u>: Esfandiar Monfaredzadeh<br/><br/>

                     <u>Abstract</u><br/>
                    Esfandiar Monfaredzadeh (Persian : اسفندیار منفردزاده) is an Iranian composer and director.
                     He was born in 1941 in Tehran His major works are Gheisar Dash Akol Tangna Gavaznha. He has
                     2 daughters Bibinaz Monfaredzadeh and Sanam Monfaredzadeh Woods (by marriage).
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Athlete<br/><br/>

                     <u>Title</u>: Elena Yakovishina<br/><br/>

                     <u>Abstract</u><br/>
                    Elena Yakovishina (born September 17 1992 in Petropavlovsk-Kamchatsky Russia) is
                     an alpine skier from Russia. She competed for Russia at the 2014 Winter Olympics
                     in the alpine skiing events.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: OfficeHolder<br/><br/>

                     <u>Title</u>: Jack Masters<br/><br/>

                     <u>Abstract</u><br/>
                    John Gerald (Jack) Masters (born September 27 1931) is a former Canadian politician.
                     He served as mayor of the city of Thunder Bay Ontario and as a federal Member of Parliament.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: MeanOfTransportation<br/><br/>

                     <u>Title</u>: HMS E35<br/><br/>

                     <u>Abstract</u><br/>
                    HMS E35 was a British E class submarine built by John Brown Clydebank.
                     She was laid down on 20 May 1916 and was commissioned on 14 July 1917.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Building<br/><br/>

                     <u>Title</u>: Aspira<br/><br/>

                     <u>Abstract</u><br/>
                    Aspira is a 400 feet (122 m) tall skyscraper in the Denny Triangle neighborhood
                     of Seattle Washington. It has 37 floors and mostly consists of apartments.
                     Construction began in 2007 and was completed in late 2009.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: NaturalPlace<br/><br/>

                     <u>Title</u>: Sierra de Alcaraz<br/><br/>

                     <u>Abstract</u><br/>
                    The Sierra de Alcaraz is a mountain range of the Cordillera Prebética located in Albacete
                     Province southeast Spain. Its highest peak is the Pico Almenara with an altitude of 1796 m.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Village<br/><br/>

                     <u>Title</u>: Piskarki<br/><br/>

                     <u>Abstract</u><br/>
                    Piskarki [pisˈkarki] is a village in the administrative district of Gmina Jeżewo
                     within Świecie County Kuyavian-Pomeranian Voivodeship in north-central Poland.
                     The village has a population of 135.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Animal<br/><br/>

                     <u>Title</u>: Lesser small-toothed rat<br/><br/>

                     <u>Abstract</u><br/>
                    The Lesser Small-toothed Rat or Western Small-Toothed Rat (Macruromys elegans) is a
                     species of rodent in the family Muridae. It is found only in West Papua Indonesia.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Plant<br/><br/>

                     <u>Title</u>: Vangueriopsis gossweileri<br/><br/>

                     <u>Abstract</u><br/>
                    Vangueriopsis gossweileri is a species of flowering plants in the family Rubiaceae.
                     It occurs in West-Central Tropical Africa (Cabinda Province Equatorial Guinea and Gabon).
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Album<br/><br/>

                     <u>Title</u>: Dreamland Manor<br/><br/>

                     <u>Abstract</u><br/>
                    Dreamland Manor is the debut album of German power metal band Savage Circus.
                     The album sounds similar to older classic Blind Guardian.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: Film<br/><br/>

                     <u>Title</u>: The Case of the Lucky Legs<br/><br/>

                     <u>Abstract</u><br/>
                    The Case of the Lucky Legs is a 1935 mystery film the third in a series of Perry
                     Mason films starring Warren William as the famed lawyer.
                 </p>
             </div>
         </center>
         </div>

         <div class="carousel-cell" style="width:100%"><center>
             <div style="width:80%; border: 0px solid #ccc;border-radius:5px;margin: 10px; padding: 4px;
    background-color: #f5f5f5;">
                 <p style="padding:10px;font-family:courier; font-size:14px;">
                     <u>Label</u>: WrittenWork<br/><br/>

                     <u>Title</u>: Everybody Loves a Good Drought<br/><br/>

                     <u>Abstract</u><br/>
                    Everybody Loves a Good Drought is a book written by P. Sainath about his research findings of
                     poverty in the rural districts of India. The book won him the Magsaysay Award.
                 </p>
             </div>
         </center>
         </div>
</div>

        </div>
        <img height="20" src="../resources/lectures/ico/paw_empty.png" style="float:left; margin-top:-10px;"/>
    <br><br>

<script>
var carousels = [document.getElementById('carousel_sst_examples'),
                    document.getElementById('carousel_imdb_examples'),
                    document.getElementById('carousel_yelp_examples'),
                    document.getElementById('carousel_amazon_examples'),
                    document.getElementById('carousel_trec_examples'),
                    document.getElementById('carousel_yahoo_examples'),
                    document.getElementById('carousel_ag_news_examples'),
                    document.getElementById('carousel_sogou_news_examples'),
                    document.getElementById('carousel_dbpedia_examples')];

var infos = [document.getElementById('dataset_info_sst'),
                    document.getElementById('dataset_info_imdb'),
                    document.getElementById('dataset_info_yelp'),
                    document.getElementById('dataset_info_amazon'),
                    document.getElementById('dataset_info_trec'),
                    document.getElementById('dataset_info_yahoo'),
                    document.getElementById('dataset_info_ag_news'),
                    document.getElementById('dataset_info_sogou_news'),
                    document.getElementById('dataset_info_dbpedia')];

function pickDatasetExample()
{
    var radioButtons = document.getElementsByName("dataset_example");

    for(var i = 0; i < radioButtons.length; i++)
    {
        if(radioButtons[i].checked == true){
            carousels[i].style.display = "block";
            infos[i].style.display = "block";
        }
        else{
            carousels[i].style.display = "none";
            infos[i].style.display = "none";
        }
    }
}

</script>

<script>

var count = 0;
var need_count = 9;

function onReadyFunction() {
  count += 1;
  if (count == need_count){ pickDatasetExample(); }
}

carousels.forEach(function(elem) {
    var flkty = new Flickity(elem,
                             { "imagesLoaded": true, "percentPosition": true, "wrapAround": true,
                             on: { ready: onReadyFunction} }
                            )})

</script>

</div>

</div>


<div id="general_view">
<h1>General View</h1>

    <p class="data_text" style="color:#888;">Here we provide a general view on classification and
        introduce the notation.
        This section applies to both classical and neural approaches.</p>


    <p>We assume that we have a collection of documents with ground-truth labels.
        The input of a classifier is a document \(x=(x_1, \dots, x_n)\) with tokens
    \((x_1, \dots, x_n)\), the output is a label \(y\in 1\dots k\).
    Usually, a classifier estimates probability distribution over classes,
    and we want the probability of the correct class to be the highest.</p>


    <div id="general_view_1">

    <h2>Get Feature Representation and Classify</h2>

        <p>Text classifiers have the following structure:</p>
        <img src="../resources/lectures/text_clf/general/idea-min.png"
             style="max-width:55%; margin-bottom:10px;margin-left:30px; float:right;"/>

            <ul>
        <li><font face="arial">feature extractor</font><br>
        A feature extractor can be
            either manually defined (as in
            <a href="#classical_approaches">classical approaches</a>)
            or learned (e.g., with <a href="#neural_approaches">neural networks</a>).
        </li>
        <li><font face="arial">classifier</font><br>
        A classifier has to assign class probabilities given the feature representation of a text.
            The most common way to do this is using <a href="#logistic_regression">logistic regression</a>,
            but other variants are also possible (e.g., <a href="#naive_bayes">Naive Bayes</a> classifier or
            <a href="#svm">SVM</a>).
        </li>
    </ul>

        <p>In this lecture, we'll mostly be looking at different ways to build feature
        representation of a text and to use this representation to get class probabilities.</p>

    </div>

    <div id="generative_discriminative">

    <h2>Generative and Discriminative Models</h2>

        <img src="../resources/lectures/text_clf/general/generative_discriminative-min.png"
             style="max-width:100%; margin-bottom:10px; "/>

        <p>A classification model can be either <font face="arial">generative</font> or
            <font face="arial">discriminative</font>.</p>
        <ul>
            <li><font face="arial">generative models</font><br>
                Generative models learn joint probability
                distribution of data \(p(x, y) = p(x|y)\cdot p(y)\). To make a prediction given
                an input \(x\), these models pick a class with the highest joint probability:
                \(y = \arg \max\limits_{k}p(x|y=k)\cdot p(y=k)\).
            </li>
            <li><font face="arial">discriminative models</font><br>
                Discriminative models are interested only in the conditional probability
                \(p(y|x)\), i.e. they learn only the border between classes. To make a prediction given
                an input \(x\), these models pick a class with the highest conditional probability:
                \(y = \arg \max\limits_{k}p(y=k|x)\).
            </li>
        </ul>

        <p>In this lecture, we will meet both generative and discriminative models.</p>

    </div>

</div>

    <br>

<div id="classical_approaches">

<h1>Classical Methods for Text Classification</h1>

    <p>In this part, we consider classical approaches for text classification. They were developed long before
    neural networks became popular, and with small datasets can still perform comparably to neural models.</p>

    <p class="data_text"><font color="#888"><u>Lena</u>: Later in the course, we will learn
    about <strong>transfer learning</strong> which can make neural approaches better even for very small datasets.
        But let's take this one step at a time: for now, classical approaches are a good baseline for your models.
    </font></p>

<div id="naive_bayes">

    <h2 style="font-size:28px;">Naive Bayes Classifier</h2>

    <p>A high-level idea of the Naive Bayes approach is given below: we rewrite
        the conditional class probability \(P(y=k|x)\) using Bayes's rule
    and get \(P(x|y=k)\cdot P(y=k)\).</p>

    <img src="../resources/lectures/text_clf/bayes/main_equation-min.png"
             style="max-width:100%; margin-bottom:10px; "/>

    <h3 style="font-size:24px;">This is a generative model!</h3>
    <center>
    <img src="../resources/lectures/text_clf/bayes/is_generative-min.png"
             style="max-width:90%; margin-bottom:10px; "/>
        </center>

    <p>Naive Bayes is a generative model: it models the joint probability of data.
    </p>
    <p>Note also the terminology:</p>
    <ul>
        <li><font face="arial"> prior</font> probability \(P(y=k)\): class
        probability before looking at data (i.e., before knowing \(x\));</li>
        <li><font face="arial"> posterior</font> probability \(P(y=k|x)\): class
        probability after looking at data (i.e., after knowing the specific \(x\));</li>
        <li><font face="arial"> joint</font> probability \(P(x, y)\): the joint
        probability of data (i.e., both examples \(x\) and labels \(y\));</li>
        <li><font face="arial">maximum a posteriori (MAP)</font> estimate: we pick the class
        with the highest posterior probability.</li>
    </ul>

    <h3 style="font-size:24px;">How to define P(x|y=k) and P(y=k)?</h3>

    <h3><u><font face="arial" color="#79a123">P(y=k)</font>: count labels</u></h3>

    <p>\(P(y=k)\) is very easy to get: we can just evaluate the proportion of documents with the label \(k\)
        (this
        is the maximum likelihood estimate, MLE).
    Namely,
    \[P(y=k)=\frac{N(y=k)}{\sum\limits_{i}N(y=i)},\]
        where \(N(y=k)\) is the number of examples (documents) with the label \(k\).
    </p>

    <h3><u><font face="arial" color="#79a123">P(x|y=k)</font>: use the "naive" assumptions, then count</u></h3>

    <p>Here we assume that document \(x\) is represented as a set of features, e.g., a set
    of its words \((x_1, \dots, x_n)\):
    \[P(x| y=k)=P(x_1, \dots, x_n|y=k).\]
    </p>

    <p><font face="arial">The Naive Bayes assumptions</font> are</p>
    <ul>
        <li><font face="arial">Bag of Words</font> assumption: word order does not matter,</li>
        <li><font face="arial">Conditional Independence</font> assumption: features (words) are independent
        given the class.</li>
    </ul>

    <p>Intuitively, we assume that the probability of each word to appear in a document with class \(k\)
        does not depend on context (neither word order nor other words at all). For example, we can say that
    <span class="data_text" style="font-weight:bold;">awesome</span>,
        <span class="data_text" style="font-weight:bold;">brilliant</span>,
        <span class="data_text" style="font-weight:bold;">great</span> are more likely to
        appear in documents with a positive sentiment and
        <span class="data_text" style="font-weight:bold;">awful</span>,
        <span class="data_text" style="font-weight:bold;">boring</span>,
        <span class="data_text" style="font-weight:bold;">bad</span>
        are more likely in negative documents, but we know nothing about how these
        (or other) words influence each other.
    </p>

    <p>With these <font face="arial">"naive"</font> assumptions we get:
    \[P(x| y=k)=P(x_1, \dots, x_n|y=k)=\prod\limits_{t=1}^nP(x_t|y=k).\]

        The probabilities \(P(x_i|y=k)\) are estimated as the proportion of times the word \(x_i\)
        appeared in documents of class \(k\) among all tokens in these documents:
        \[P(x_i|y=k)=\frac{N(x_i, y=k)}{\sum\limits_{t=1}^{|V|}N(x_t, y=k)},\]
        where \(N(x_i, y=k)\) is the number of times
        the token \(x_i\) appeared in documents
        with the label \(k\), \(V\) is the vocabulary (more generally, a set of all possible features).
    </p>

    <h3><font face="arial">What if \(N(x_i, y=k)=0\)?  Need to avoid this!</font></h3>

        <p>What if \(N(x_i, y=k)=0\), i.e. in training we haven't seen the token \(x_i\) in the documents
        with class \(k\)?
        This will null out the probability of the whole document,
        and this is not what we want! For example if we haven't seen some rare words
        (e.g., <span class="data_text" style="font-weight:bold;">pterodactyl</span>
        or <span class="data_text" style="font-weight:bold;">abracadabra</span>)
    in training positive examples, it does not mean that a positive document can never
        contain these words.
    </p>

<img src="../resources/lectures/text_clf/bayes/need_smoothing-min.png"
             style="max-width:100%; margin-bottom:10px; "/>

<p>To avoid this, we'll use a simple trick: we add to counts of all words a small \(\delta\):
\[P(x_i|y=k)=\frac{\color{red}{\delta} +\color{black} N(x_i, y=k)
    }{\sum\limits_{t=1}^{|V|}(\color{red}{\delta} +\color{black}N(x_t, y=k))} =
    \frac{\color{red}{\delta} +\color{black} N(x_i, y=k)
    }{\color{red}{\delta\cdot |V|}\color{black}  + \sum\limits_{t=1}^{|V|}\color{black}N(x_t, y=k)}
    ,\]
    where \(\delta\) can be chosen using cross-validation.
</p>

    <p><u>Note</u>: this is <font face="arial">Laplace smoothing</font>
        (aka <font face="arial">Add-1 smoothing</font> if \(\delta=1\)). We'll learn more about smoothings
    in the next lecture when talking about Language Modeling.</p>

    <h3 style="font-size:24px;">Making a Prediction</h3>

    <p>As we already mentioned,
        Naive Bayes (and, more broadly, generative models) make
        a prediction based on the joint probability of
    data and class:
        \[y^{\ast} = \arg \max\limits_{k}P(x, y=k) = \arg \max\limits_{k} P(y=k)\cdot P(x|y=k).\]
    </p>

     <p>Intuitively, Naive Bayes expects that some words serve as class indicators.
    For example, for sentiment classification tokens
    <span class="data_text" style="font-weight:bold;">awesome</span>,
        <span class="data_text" style="font-weight:bold;">brilliant</span>,
        <span class="data_text" style="font-weight:bold;">great</span>
        will have a higher probability given positive class than negative. Similarly,
        tokens
        <span class="data_text" style="font-weight:bold;">awful</span>,
        <span class="data_text" style="font-weight:bold;">boring</span>,
        <span class="data_text" style="font-weight:bold;">bad</span>
        will have a higher probability given negative class than positive.
    </p>

    <center>
        <img src="../resources/lectures/text_clf/bayes/example-min.png"
             style="max-width:90%; margin-bottom:10px; "/>
    </center>


    <br><br>
    <h3 style="font-size:24px;">Final Notes on Naive Bayes</h3>

    <h3><u>Practical Note</u>: Sum of Log-Probabilities Instead of Product of Probabilities</h3>

    <p>The main expression  Naive Bayes uses for classification is a product lot of probabilities:
    \[P(x, y=k)=P(y=k)\cdot P(x_1, \dots, x_n|y)=P(y=k)\cdot \prod\limits_{t=1}^nP(x_t|y=k).\]
    A product of many probabilities may be very unstable numerically.
        Therefore, usually instead of \(P(x, y)\) we consider
        \(\log P(x, y)\):
        \[\log P(x, y=k)=\log P(y=k) + \sum\limits_{t=1}^n\log P(x_t|y=k).\]
        Since we care only about argmax, we can consider \(\log P(x, y)\) instead of \(P(x, y)\).
    </p>

    <p><u>Important!</u> Note that in practice, we will usually deal with log-probabilities and not
    probabilities.</p>


    <h3><u>View in the General Framework</u></h3>
    <img src="../resources/lectures/text_clf/bayes/bayes_features-min.png"
             style="max-width:55%; margin-bottom:10px;float:right; margin-left:25px;"/>
    <p>Remember our <a href="#general_view">general view</a> on the classification task?
    We obtain feature representation of the input text using some method, then use this
        feature representation for classification.
    </p>
    <p>In Naive Bayes, our features are words, and the feature representation is the Bag-of-Words (BOW)
    representation - a sum of one-hot representations of words. Indeed,
        to evaluate \(P(x, y)\)
        we only need to number of times each token appeared in the text.
    </p>

    <h3><u>Feature Design</u></h3>

    <p>In the standard setting, we used words as features. However, you can use other types of features:
    URL, user id, etc.</p>

    <div class="card_with_ico">
    <img class="ico" src="../resources/lectures/ico/bulb_empty.png"/>
    <div class="text_box_yellow">
    <p class="data_text">
        Even if your data is plain text (without fancy things such as URL, user id, etc),
        you can still design features in different ways.
        Learn how to improve Naive Bayes in <a href="#research_improve_bayes">this exercise</a>
    in the <a href="#research_thinking">Research Thinking</a> section. </p>
    </div>
    </div>


</div>

<div id="logistic_regression">

    <h2 style="font-size:28px;">Maximum Entropy Classifier (aka Logistic Regression)</h2>

    <p>Differently from Naive Bayes, MaxEnt classifier is a discriminative model, i.e., we
    are interested in \(P(y=k|x)\) and not in the joint distribution \(p(x, y)\).
    Also, we will <font face="arial">learn</font> how to use features: this is in contrast to
    Naive Bayes, where we defined how to use the features ourselves.</p>

    <p>Here we also have to define features manually, but we have more freedom:
        features do not have to be categorical (in Naive Bayes, they had to!).
        We can use the BOW representation or
    come up with something more interesting.</p>

    <p>The general classification pipeline here is as follows:</p>
    <ul>
        <li>get \(\color{#7aab00}{h}\color{black}=(\color{#7aab00}{f_1}\color{black},
            \color{#7aab00}{f_2}\color{black}, \dots,
            \color{#7aab00}{f_n}\color{black}{)}\) -
            feature representation of the input text;</li>
        <li>take \(w^{(i)}=(w_1^{(i)}, \dots, w_n^{(i)})\)
            <!--\(\color{#ebb802}{w^{(1)}}\), \(\color{#991a87}{w^{(2)}}\), ...,
        \(\color{#027dbf}{w^{(K)}}\)--> - vectors with feature weights
            for each of the classes;
        </li>
        <li>for each class, weigh features, i.e. take the dot product of feature representation \(\color{#7aab00}{h}\) with
        feature weights \(w^{(k)}\):  <!--\(\color{#ebb802}{w^{(1)}}\), \(\color{#991a87}{w^{(2)}}\), ...,
        \(\color{#027dbf}{w^{(K)}}\).-->
            <!--\[\color{#991a87}{w^{(2)}}\color{#7aab00}{h} =
            \color{#991a87}{w_1^{(2)}}\color{black}{\cdot}\color{#7aab00}{f_1}\color{black}{+\dots+}
            \color{#991a87}{w_n^{(2)}}\color{black}{\cdot}\color{#7aab00}{f_n},
            \]-->
            \[w^{(k)}\color{#7aab00}{h}\color{black} =
            w_1^{(k)}\cdot\color{#7aab00}{f_1}\color{black}+\dots+
            w_n^{(k)}\cdot\color{#7aab00}{f_n}\color{black}{, \ \ \ \ \ k=1, \dots, K.}
            \]
            To get a bias term in the sum above, we define one of the features being 1
            (e.g., \(\color{#7aab00}{f_0}=1\)). Then
            \[w^{(k)}\color{#7aab00}{h}\color{black} = \color{red}{w_0^{(k)}}\color{black} +
            w_1^{(k)}\cdot\color{#7aab00}{f_1}\color{black}+\dots+
            w_n^{(k)}\cdot\color{#7aab00}{f_{n}}\color{black}{, \ \ \ \ \ k=1, \dots, K.}
            \]
        </li>
        <li>get class probabilities using softmax:
        \[P(class=k|\color{#7aab00}{h}\color{black})=
            \frac{\exp(w^{(k)}\color{#7aab00}{h}\color{black})}{\sum\limits_{i=1}^K
            \exp(w^{(i)}\color{#7aab00}{h}\color{black})}.\]
            Softmax normalizes the \(K\) values we got at the previous step
            to a probability distribution over output classes.
        </li>
    </ul>

    <p>Look at the illustration below (classes are shown in different colors).</p>

    <center>
    <img src="../resources/lectures/text_clf/maxent/idea-min.png"
             style="max-width:90%; margin-bottom:10px;"/>
    </center>



    <br><br>
    <h3 style="font-size:24px;">Training: Maximum Likelihood Estimate</h3>

    <p>Given training examples \(x^1, \dots, x^N\) with labels \(y^1, \dots, y^N\), \(y^i\in\{1, \dots, K\}\),
    we pick those weights \(w^{(k)}, k=1..K\) which maximize the probability of the training data:
        \[w^{\ast}=\arg \max\limits_{w}\sum\limits_{i=1}^N\log P(y=y^i|x^i).\]
        In other words, we choose parameters such that the data is <font face="arial">more likely</font>
        to appear. Therefore, this is called the <font face="arial">Maximum Likelihood Estimate (MLE)
        of the parameters.</font>
    </p>

    <p>To find the parameters maximizing the data log-likelihood, we use gradient ascent:
    gradually improve weights during multiple iterations over the data.
    At each step, we maximize the probability a model assigns to the correct class.
    </p>

    <!-- <font color="red">add illustration of MLE (similar to LM evaluation)?</font> -->

    <h3 id="max_mle_min_xent"><u>Equvalence to minimizing cross-entropy</u></h3>
    <p>Note that maximizing data log-likelihood is equivalent to minimizing  cross entropy
    between the target probability distribution \(p^{\ast} = (0, \dots, 0, 1, 0, \dots)\)
        (1 for the target label, 0 for the rest) and the predicted by the model
    distribution \(p=(p_1, \dots, p_K), p_i=p(i|x)\):
    \[Loss(p^{\ast}, p^{})= - p^{\ast} \log(p) = -\sum\limits_{i=1}^{K}p_i^{\ast} \log(p_i).\]
            Since only one of \(p_i^{\ast}\) is non-zero (1 for the target label \(k\), 0 for the rest), we will get
            \(Loss(p^{\ast}, p) = -\log(p_{k})=-\log(p(k| x)).\)
    </p>

    <img src="../resources/lectures/text_clf/maxent/mle_cross_entropy-min.png"
             style="max-width:90%; margin-bottom:10px;"/>

    <p>This equivalence is very important for you to understand: when talking about neural approaches,
        people usually say that they minimize the cross-entropy loss.
        Do not forget that this is the same as maximizing the data log-likelihood.
    </p>


    <h3 style="font-size:24px;">Naive Bayes vs Logistic Regression</h3>

<!--
    <div style="display:grid;grid-template-columns: 50% 50%;">
        <div>
            <p><u>Naive Bayes:</u></p>
            <ul class="color:red">

                <li>something</li>
            </ul>
        </div>

    </div>
-->
    <img src="../resources/lectures/text_clf/maxent/bayes_vs_maxent-min.png"
             style="max-width:90%; margin-bottom:10px;"/>
    <p>Let's finalize this part by discussing the advantages and drawbacks of logistic regression and Naive Bayes.</p>
    <ul>
        <li><font face="arial">simplicity</font><br>
        Both methods are simple; Naive Bayes is the simplest one.
        </li>
        <li><font face="arial">interpretability</font><br>
        Both methods are interpretable: you can look at the features which influenced the predictions most
            (in Naive Bayes - usually words, in logistic regression - whatever you defined).
        </li>
        <li><font face="arial">training speed</font><br>
        Naive Bayes is very fast to train - it requires only one pass through the training data to evaluate the counts.
            For logistic regression, this is not the case: you have to go over the data many times
            until the gradient ascent converges.
        </li>
        <li><font face="arial">independence assumptions</font><br>
        Naive Bayes is too "naive" - it assumed that features (words) are conditionally independent given class.
            Logistic regression does not make this assumption - we can hope it is better.
        </li>
        <li><font face="arial">text representation: manual</font><br>
        Both methods use manually defined feature representation (in Naive Bayes, BOW is the standard choice,
            but you still choose this yourself). While manually defined features are good for interpretability,
            they may be not so good for performance - you are likely to miss something which can be useful for the task.
        </li>

    </ul>

</div>


<div id="svm">
<h2 style="font-size:24px;">SVM for Text Classification</h2>
    <img src="../resources/lectures/text_clf/svm/main-min.png"
             style="max-width:60%; margin-bottom:10px; float:right; margin-left:20px;"/>
    <p>One more method for text classification based on  manually designed features is SVM.
        The most basic (and popular) features for SVMs are <font face="arial">bag-of-words</font> and
        <font face="arial">bag-of-ngrams</font> (<font face="arial">ngram</font> is a tuple of n words).
        With these simple features, SVMs with linear kernel perform better than Naive Bayes
        (see, for example, the paper
        <a href="https://www.comp.nus.edu.sg/~leews/publications/p31189-zhang.pdf" target="_blank">Question
            Classification using Support Vector Machines</a>).
    </p>

</div>

</div>




<br><br>
<div id="neural_approaches">

    <h1>Text Classification with Neural Networks</h1>

    <div id="nn_high_level">


        <div class="green_left_thought" style="font-size:18px;">
        <p  class="data_text">
            Instead of manually defined features,
            let a neural network to
            <strong>learn useful features</strong>.</p>
        </div>

        <p>The main idea of neural-network-based classification is that feature representation of the input text
        can be obtained using a neural network. In this setting, we feed the embeddings of the input tokens
        to a neural network, and this neural network gives us a vector representation of the input text.
        After that, this vector is used for classification.</p>
        <center>
        <img src="../resources/lectures/text_clf/neural/general_vs_nns_linear-min.png"
             style="max-width:100%; margin-bottom:20px; "/>
        </center>

        <p>When dealing with neural networks, we can think about the classification part
        (i.e., how to get class probabilities from a vector representation of a text) in a very simple way.</p>
        <img src="../resources/lectures/text_clf/neural/classification_part_explained-min.png"
             style="max-width:50%; margin-left:20px; float:right;"/>

        <p>
            Vector representation of a text has some dimensionality \(d\), but in the end,
            we need a vector of size \(K\) (probabilities for \(K\) classes).
                To get a \(K\)-sized vector from a \(d\)-sized,
            we can use a linear layer. Once we have a \(K\)-sized vector, all is left is to
            apply the softmax operation to convert the raw numbers into class probabilities.
        </p>

        <h3>Classification Part: This is Logistic Regression!</h3>
        <p>Let us look closer at the neural network classifier. The way we use vector representation
            of the input text is exactly the same as we did with logistic regression:
            we weigh features according to feature weights for each class.
            <font face="arial">The only difference
                from logistic regression</font>
            is where the features come from: they are either defined manually (as we did before)
            or obtained by a neural network.
        </p>

        <center>
        <img src="../resources/lectures/text_clf/neural/nn_linear_with_logreg-min.png"
             style="max-width:100%; margin-bottom:20px; "/>
        </center>


        <h3>Intuition: Text Representation Points in the Direction of Class Representation</h3>

        <center>
        <img src="../resources/lectures/text_clf/neural/linear_layer_intuition-min.png"
             style="max-width:100%; margin-bottom:20px; "/>
        </center>

        <p>If we look at this final linear layer more closely, we will see that the columns
            of its matrix are
            vectors \(w_i\). These vectors can be thought of as vector representations of classes.
            A good neural network will learn to represent input texts in such a way that
            text vectors will point in the direction of the corresponding class vectors.
        </p>

        <!--
        <div class="green_left_thought" style="font-size:18px;">
                <p  class="data_text">
                    Instead of manually defined features,
                    let a neural network to
                    <strong>learn useful features</strong>.</p>
            </div>

        <p>The main idea of neural-network-based classification is that feature representation of the input text
        can be obtained using a neural network. In this setting, we feed the embeddings of the input tokens
        to a neural network, and this neural network gives us a vector representation of the input text.
        After that, this vector is used for classification (usually with logistic regression).</p>
        <center>
        <img src="../resources/lectures/text_clf/neural/general_vs_nns-min.png"
             style="max-width:100%; margin-bottom:20px; "/>
        </center>

        <p>Let us look closer into the neural network classifier. The way we use vector representation
        of the input text is exactly the same as we did with logistic regression:
            we weigh features according to feature weights for each class. <font face="arial">The only difference
                from logistic regression</font>
        is where the features come from: they are either defined manually (as we did before)
        or obtained by a neural network.</p>

        <center>
        <img src="../resources/lectures/text_clf/neural/nn_logreg_with_linear-min.png"
             style="max-width:100%; margin-bottom:20px; "/>
        </center>

        <p>When dealing with neural networks, we can think about the classification part
        (i.e., how to get class probabilities from a vector representation of a text) in a more simple way.
        Vector representation of a text has some dimensionality \(d\), but in the end
        we need a vector of size \(K\) (probabilities for \(K\) classes).
            To get a \(K\)-sized vector from a \(d\)-sized,
        we can use a linear layer. Once we have a \(K\)-sized vector, all is left is to
        apply the softmax operation to convert the raw numbers into class probabilities.</p>

        <center>
        <img src="../resources/lectures/text_clf/neural/linear_layer_intuition-min.png"
             style="max-width:100%; margin-bottom:20px; "/>
        </center>

        <p>If we look at this final linear layer more closely, we will see that the columns
            of its matrix are
            vectors \(w_i\). These vectors can be though of as vector representations of classes.
            A good neural network will learn to represent input texts in such a way that
            text vectors will point in the direction of the corresponding class vectors.
        </p>
        -->
<!--
        <img src="../resources/lectures/text_clf/neural/linear_layer_details-min.png"
             style="max-width:40%; margin-bottom:10px; float:right;"/>
        <br><br><br><br><br>
        <img src="../resources/lectures/text_clf/neural/nn_learns_intuition-min.png"
             style="max-width:30%; margin-bottom:10px; float:right; "/>
-->


    </div>

    <div id="nn_training">
    <h2>Training and the Cross-Entropy Loss</h2>

        <p>Neural classifiers are trained to predict probability distributions
            over classes.
            Intuitively, at each step, we maximize the probability a model assigns to the correct class.</p>

        <p>The standard loss function is the <font face="arial">cross-entropy loss</font>.
        Cross-entropy loss for the target probability distribution \(p^{\ast} = (0, \dots, 0, 1, 0, \dots)\)
        (1 for the target label, 0 for the rest) and the predicted by the model
    distribution \(p=(p_1, \dots, p_K), p_i=p(i|x)\):
    \[Loss(p^{\ast}, p^{})= - p^{\ast} \log(p) = -\sum\limits_{i=1}^{K}p_i^{\ast} \log(p_i).\]
            Since only one of \(p_i^{\ast}\) is non-zero (1 for the target label \(k\), 0 for the rest), we will get
            \(Loss(p^{\ast}, p) = -\log(p_{k})=-\log(p(k| x)).\)
            Look at the illustration for one training example.
        </p>

        <center>
        <img src="../resources/lectures/text_clf/neural/cross_entropy-min.png"
             style="max-width:100%; margin:20px;"/>
        </center>



        <p>In training, we
            gradually improve model weights during multiple iterations over the data:
            we iterate over training examples (or batches of examples) and
            make gradient updates.
            At each step, we maximize the probability a model assigns to the correct class.
            At the same time, we minimize the sum of the probabilities of incorrect classes:
            since the sum of all probabilities is constant, by increasing one probability we decrease
            the sum of all the rest
            (<span class="data_text"><font color="#888"><u>Lena</u>:
                Here I usually imagine a bunch of kittens eating from the same bowl:
                one kitten always eats at the expense of the others</font></span>).</p>

            <p>Look at the illustration of the training process.
        </p>

     <center>
     <video width="70%" height="auto" loop autoplay muted style="margin-left: 20px;">
         <source src="../resources/lectures/text_clf/neural/nn_clf_training.mp4" type="video/mp4">
     </video>
     </center>


     <br>
     <h3>Recap: This is equivalent to maximizing the data likelihood</h3>
         <p>Do not forget that <a href="#max_mle_min_xent">when talking about MaxEnt classifier
             (logistic regression)</a>,
             we showed that minimizing cross-entropy is equivalent to
             maximizing the data likelihood. Therefore, here we are also
             trying to get the <font face="arial">Maximum Likelihood Estimate (MLE)
        of model parameters.</font>
         </p>
    </div>



<br>
<h2 id="nn_models" style="font-size:28px;">Models for Text Classification</h2>

    <div class="green_left_thought" style="font-size:18px;">
                <p class="data_text">
                    We need a model
                    that can produce a <strong>fixed-sized</strong> vector
                    for inputs of <strong>different</strong> lengths.
                </p>
            </div>

    <img src="../resources/lectures/text_clf/neural/model_template-min.png"
             style="max-width:50%; margin-bottom:20px; margin-left:20px; float:right; "/>

    <p>In this part, we will look at different ways to get a vector representation of an input text
        using neural networks. Note that while input texts can have different lengths,
        the vector representation of a text has to
        have a fixed size: otherwise,
        a network will not "work".
    </p>

    <p>We begin with the simplest approaches which use only word embeddings (without adding a model
        on top of that). Then we look at recurrent and convolutional networks. </p>

    <p class="data_text"><font color="#888">
      <u>Lena:</u>
        A bit later in the course, you will learn about Transformers and the most recent classification techniques
    using large pretrained models. </font></p>

    <div id="nn_models_simple">
    <h2>Basics: Bag of Embeddings (BOE) and Weighted BOE</h2>

        <p>The simplest you can do is use only word embeddings without any neural network on top of that.
        To get vector representation of a text, we can either sum all token embeddings (Bag of Embeddings) or
            use a weighted sum of these embeddings (with weights, for example, being tf-idf or something else).

        </p>

        <p>Bag of Embeddings (ideally, along with Naive Bayes) should be a baseline for any model with a
            neural network:
        if you can't do better than that, it's not worth using NNs at all. This can be the case if you don't
        have much data.</p>

        <center>
        <img src="../resources/lectures/text_clf/neural/bow_tfidf-min.png"
             style="max-width:100%; margin-bottom:20px; "/>
        </center>
        <p>While Bag of Embeddings (BOE) is sometimes called Bag of Words (BOW),
            note that <font face="arial">these two are very different</font>.
            BOE is the sum of embeddings and BOW
            is the sum of one-hot vectors:
            BOE knows a lot more about language. The pretrained embeddings (e.g., Word2Vec or GloVe)
            understand similarity between words. For example,
            <span class="data_text" style="font-weight:bold;">awesome</span>,
        <span class="data_text" style="font-weight:bold;">brilliant</span>,
        <span class="data_text" style="font-weight:bold;">great</span> will be represented with
            unrelated features in BOW but similar word vectors in BOE.
        </p>
        <p>Note also that to use a weighted sum of embeddings, you need to come up with a way to get weights.
            However, this is exactly what we wanted to avoid by using neural networks: we don't want
            to introduce manual features, but rather let a network learn useful patterns.</p>

        <h3>Bag of Embeddings as Features for SVM</h3>
        <p>You can use SVM on top of BOE!
            The only difference from SVMs in classical
        approaches (on top of bag-of-words and bag-of-ngrams)
            if the choice of a kernel: here the RBF kernel is better.
        </p>
    </div>


    <div id="nn_models_rnn">
    <h2>Models: Recurrent (RNN/LSTM/etc)</h2>
        <p>Recurrent networks are a natural way to process text in a sense that, similar to humans,
        they "read" a sequence of tokens one by one and process the information. Hopefully,
        at each step, the network will "remember" everything it has read before.</p>

        <!--
        <div style="display:grid;grid-template-columns: 30% 70%;">
            <div>
                <img src="../resources/lectures/text_clf/neural/rnn/general_cell-min.png"
             style="max-width:100%; margin-bottom:20px;"/>
            </div>
            <div>
                <video width="90%" height="auto" loop autoplay muted style="margin-left: 20px;float:right;">
                 <source src="../resources/lectures/text_clf/neural/rnn/rnn_reads_text.mp4" type="video/mp4">
                </video>
            </div>
        </div>
        -->

        <h3><u>Basics: Recurrent Neural Networks</u></h3>
        <div style="display:grid;grid-template-columns: 65% 35%;">
            <div>
                <h4 style="font-size:18px;"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                    <font face="arial">RNN cell</font></h4>
                <p>At each step, a recurrent network receives a new input vector (e.g., token embedding) and
            the previous network state (which, hopefully, encodes all previous information).
            Using this input, the RNN cell computes the new state which it gives as output.
            This new state now contains information about
            both current
            input and the information from previous steps.
        </p>
            </div>
            <div>
                <img src="../resources/lectures/text_clf/neural/rnn/general_cell-min.png"
             style="max-width:90%; margin-bottom:20px; margin-left:20px; float:right;"/>
            </div>
        </div>


        <div style="display:grid;grid-template-columns: 40% 60%;">
            <div>
                <h4 style="font-size:18px;"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                    <font face="arial">RNN reads a sequence of tokens</font></h4>
            <p>Look at the illustration: RNN reads a text token by token, at each step using
            a new token embedding and the previous state.</p>
            <p>Note that the <font face="arial">RNN cell is the same</font> at each step!</p>

            </div>
            <div>
                <video width="95%" height="auto" loop autoplay muted style="margin-bottom: 20px; margin-left: 10px; float:right;">
             <source src="../resources/lectures/text_clf/neural/rnn/rnn_reads_text.mp4" type="video/mp4">
        </video>
            </div>
        </div>


         <div style="display:grid;grid-template-columns: 60% 40%;">
            <div>
                <h4 style="font-size:18px;"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                    <font face="arial">Vanilla RNN</font></h4>
                <p>The simplest recurrent network, <font face="arial">Vanilla RNN</font>,
                transforms \(h_{t-1}\) and \(x_t\) linearly, then applies a non-linearity (most often,
                    the \(\tanh\) function):
                    \[h_t = \tanh(h_{t-1}W_h + x_tW_t).\]
                </p>
            </div>
            <div>
                <img src="../resources/lectures/text_clf/neural/rnn/vanilla_cell-min.png"
             style="max-width:90%; margin-bottom:20px; float:right;"/>
            </div>
        </div>

        <p>Vanilla RNNs suffer from the <font face="arial">vanishing and exploding gradients</font> problem.
        To alleviate this problem, more complex recurrent cells (e.g., LSTM, GRU, etc) perform
            several operations on the input and use
            gates. For more details of RNN basics, look at the
            <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">Colah's blog post</a>.
        </p>


        <h3><u>Recurrent Neural Networks for Text Classification</u></h3>
         <p>Here we (finally!) look at how we can use recurrent models for text classification.
            Everything you will see here will apply to all recurrent cells,
             and by "RNN" in this part I refer to recurrent cells in general (e.g. vanilla RNN, LSTM, GRU, etc).
         </p>

        <p>Let us recall what we need:</p>
        <div class="green_left_thought" style="font-size:18px;">
                <p class="data_text">
                    We need a model
                    that can produce a <strong>fixed-sized</strong> vector
                    for inputs of <strong>different</strong> lengths.
                </p>
            </div>

        <div style="display:grid;grid-template-columns: 40% 60%;">
            <div>
                <h4 style="font-size:18px;"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                        <font face="arial">Simple</font>: read a text, take the final state</h4>
                    <p>The most simple recurrent model is a one-layer RNN network.
                    In this network, we have to take the state which knows more about input text.
                        Therefore, we have to
                        use the last state - only this state saw all input tokens.
                    </p>
            </div>
            <div>
                <img src="../resources/lectures/text_clf/neural/rnn/rnn_final_state-min.png"
             style="max-width:90%; margin-left:20px; float:right;"/>
            </div>
        </div>


        <div style="display:grid;grid-template-columns: 50% 50%;">
            <div>
                 <h4 style="font-size:18px;"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                <font face="arial">Multiple layers</font>: feed the states from one RNN to the next one</h4>
                <p>To get a better text representation, you can stack multiple layers. In this case,
                inputs for the higher RNN are representations coming from the previous layer.
                </p>

                <p>The main hypothesis is that with several layers,
                    lower layers will catch local phenomena (e.g., phrases), while
                    higher layers will be able to learn more high-level things (e.g., topic).
                </p>
            </div>
            <div>
                <img src="../resources/lectures/text_clf/neural/rnn/multi_layer-min.png"
             style="max-width:90%; margin-left:20px; float:right;"/>
            </div>
        </div>

        <h4 style="font-size:18px;"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
            <font face="arial">Bidirectional</font>: use final states from forward and backward RNNs.</h4>

        <p>Previous approaches may have a problem: the last state can easily "forget" earlier
        tokens. Even strong models such as LSTMs can still suffer from that!</p>
        <p>To avoid this, we can use two RNNs:
            <font face="arial" color="#d192ba">forward</font>, which reads input from left to right, and
        <font face="arial" color="#88bd33">backward</font>, which reads input from right to left. Then we can use the final states from both models:
        one will better remember the final part of a text, another - the beginning. These states
        can be concatenated, or summed, or something else - it's your choice!</p>
        <center>
        <img src="../resources/lectures/text_clf/neural/rnn/bidirectional-min.png"
             style="max-width:75%; margin-bottom:20px; "/>
        </center>

        <h4 style="font-size:18px;"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
            <font face="arial">Combinations</font>: do everything you want!</h4>
        <p>You can combine the ideas above. For example, in a multi-layered network, some
        layers can go in the opposite direction, etc.</p>

    </div>



    <div id="nn_models_cnn">
    <h2>Models: Convolutional (CNN)</h2>

        <p class="data_text"><font color="#888">The detailed description of convolutional models, in general, is in
        <a href="./models/convolutional.html" target="_blank">Convolutional Models Supplementary</a>.
            In this part, we consider only convolutions for text classification.</font>
        </p>

        <h4 style="font-size:18px;">
                <font face="arial">Convolutions for Images and Translation Invariance</font></h4>
            <p>Convolutional networks were originally developed for computer vision tasks. Therefore, let's first
    understand the intuition behind convolutional models for images.</p>
    <p>Imagine we want to classify an image into several classes, e.g. cat, dog, airplane, etc.
        In this case, if you find a cat on an image, you <font face="arial">don't care where</font> on the image
        this cat is: you care only that it is there somewhere.
    </p>

        <center>
        <img src="../../resources/lectures/models/cnn/translation_cats-min.png"
             style="max-width:100%; margin-bottom:20px;"/>
        </center>
    <p>
    </p>

    <div style="display:grid;grid-template-columns: 75% 25%;">

        <div>
            <p>Convolutional networks apply the same operation to small parts of an image: this is how they extract features.
                Each operation is looking for a match with a pattern,
                and a network learns which patterns are useful.
                With a lot of layers, the learned patterns become more and more complicated: from lines in the early layers
                to very complicated patterns (e.g., the whole cat or dog) on the upper ones. You can look at the examples
                in the <a href="#analysis_interpretability">Analysis and Interpretability</a> section.
            </p>
            <p>This property is called <font face="arial">translation invariance</font>:
                <font face="arial">translation</font> because we are talking about shifts in space,
                <font face="arial">invariance</font> because we want it to not matter.
            </p>
        </div>
        <div>
            <p style="text-align: center; float: right; display: block; margin-left:25px;
                   margin-top:-10px; line-height:15px;">
                <video width="100%" height="auto" loop autoplay muted style="margin-left: 20px;">
                <source src="../../resources/lectures/models/cnn/cnn_with_cat.mp4" type="video/mp4">
                </video>
                <br />
                <span style="font-size: small;">The illustration is adapted from the one taken from
                <a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank">this cool repo</a>.</span>
            </p>
        </div>
    </div>

        <h4 style="font-size:18px;">
                <font face="arial">Convolutions for Text</font></h4>
        <p>Well, for images it's all clear: e.g. we want to be able to move a cat because we don't care where the cat is.
    But what about texts? At first glance, this is not so straightforward: we can not move phrases easily -
        the meaning will change or we will get something that does not make much sense.
    </p>

    <p>However, there are some applications where we can think of the same intuition. Let's imagine
    that we want to classify texts, but not cats/dogs as in images, but positive/negative sentiment.
    Then there are some words and phrases which could be very informative "clues" (e.g.
        <font class="data_text"><strong>it's been great</strong></font>,
        <font class="data_text"><strong>bored to death</strong></font>,
        <font class="data_text"><strong>absolutely amazing</strong></font>,
        <font class="data_text"><strong>the best ever</strong></font>, etc), and others which are not important at all.
        We don't care much where in a text we saw
        <font class="data_text"><strong>bored to death</strong></font> to understand the sentiment, right?
    </p>

        <center>
        <img src="../../resources/lectures/models/cnn/translation_texts-min.png"
             style="max-width:100%; margin-bottom:20px;"/>
        </center>



        <h4 style="font-size:18px;">
                <font face="arial">A Typical Model: Convolution+Pooling Blocks</font></h4>
            <p>Following the intuition above, we want to detect some patterns, but we don't
        care much about where exactly these patterns are. This behavior is implemented with
        two layers:</p>
    <ul>
        <li><font face="arial">convolution</font>: finds matches with patterns (as the cat head we saw above);</li>
        <li><font face="arial">pooling</font>: aggregates these matches over positions (either locally or globally).</li>
    </ul>

        <p>A typical convolutional model for text classification is shown in the figure.
            To get a vector representation of an input text,
            a convolutional layer is applied to word embedding,
            which is followed by a non-linearity
            (usually ReLU) and
            a pooling operation. The way this representation is used for classification is similar to other networks.
        </p>
        <center>
        <img src="../resources/lectures/text_clf/neural/cnn/model_general-min.png"
             style="max-width:100%; margin-bottom:20px;"/>
        </center>
        <p>In the following, we discuss in detail the main building blocks, convolution and pooling,
        then consider modeling modifications.</p>


        <h3><u>Basics: Convolution Layer for Text</u></h3>


        <div style="display:grid;grid-template-columns: 75% 25%;">

            <div>
                <p>Convolutional Neural Networks were initially developed for computer vision tasks, e.g. classification
                    of images (cats vs dogs, etc). The idea of a convolution is to go over an image with a
                    sliding window and to apply the same operation,
                    <font face="arial">convolution filter</font>, to each window.</p>

                <p>The illustration (taken from <a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank">this cool repo</a>)
                shows this process for one filter: the bottom is the input image, the top is the filter output.
                    Since an image has two dimensions (width and height), the convolution is two-dimensional.
                </p>
            </div>
            <div>
                <p style="text-align: center; float: right; display: block; margin-left:25px;
                            margin-top:-10px; line-height:15px;">
                    <img src="../resources/lectures/models/cnn/same_padding_no_strides.gif"/><br />
                    <span style="font-size: small;">Convolution filter for images. The illustration is from
                    <a href="https://github.com/vdumoulin/conv_arithmetic" target="_blank">this cool repo</a>.</span>
                </p>
            </div>
        </div>


        <div style="display:grid;grid-template-columns: 50% 50%;">
            <div>
                 <p>Differently from images, texts have only one dimension: here a convolution is one-dimensional:
                 look at the illustration.
                 </p>

            </div>
            <div>
                <video width="90%" height="auto" loop autoplay muted style="margin-left: 20px;float:right;">
                <source src="../resources/lectures/models/cnn/cnn_filter_reads_text.mp4" type="video/mp4">
                </video>
                <p style="font-size:14px;text-align:center;margin-top:-20px;">Convolution filter for text.</p>
            </p>

            </div>
        </div>


        <h4 style="font-size:18px;">
                <font face="arial">Convolution is a Linear Operation Applied to Each Window</font></h4>
                <img src="../../resources/lectures/models/cnn/convolution_linear_layer-min.png"
             style="max-width:100%; margin-bottom:20px;"/>
        <p>A convolution is a linear layer (followed by a non-linearity) applied to each input window.
            Formally, let us assume that
        </p>
        <ul>
            <li>\((x_1, \dots, x_n)\) - representations of the input words, \(x_i\in \mathbb{R}^d\);</li>
            <li>\(d\) (<font face="arial">input channels</font>) - size of an input embedding;</li>
            <li>\(k\) (<font face="arial">kernel size</font>) - the length of a convolution window (on the illustration, \(k=3\));</li>
            <li>\(m\) (<font face="arial">output channels</font>) - number of convolution filters
                (i.e., number of channels produced by the convolution).</li>
        </ul>
        <p>Then a convolution is a linear layer \(W\in\mathbb{R}^{(k\cdot d)\times m}\).
         For a \(k\)-sized window \((x_i, \dots x_{i+k-1})\), the convolution
                    takes the concatenation of these vectors
                    \[u_i = [x_i, \dots x_{i+k-1}]\in\mathbb{R}^{k\cdot d}\]
                    and multiplies by the convolution matrix:
            \[F_i = u_i \times W.\]
            A convolution goes over an input
            with a sliding window and applies
            the same linear transformation to each window.
        </p>


        <h4 style="font-size:18px;">
                <font face="arial"><u>Intuition</u>: Each Filter Extracts a Feature</font></h4>
        <p>Intuitively, each filter in a convolution extracts a feature.</p>
        <img src="../resources/lectures/models/cnn/one_filter-min.png"
             style="max-width:50%; margin-left:20px; float:right;"/>
        <h3><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                <font face="arial">One filter</font> - one feature extractor</h3>
                <p>A filter takes vector representations in a current window

                and transforms them linearly into a single feature. Formally,
                    for a window \(u_i = [x_i, \dots x_{i+k-1}]\in\mathbb{R}^{k\cdot d}\)
                    a filter
                    \(f\in\mathbb{R}^{k\cdot d}\) computes dot product:
                    \[F_i^{(f)} = (f, u_i).\]
                    The number \(F_i^{(f)}\) (the extracted "feature") is a result of applying the
                    filter \(f\) to the window \((x_i, \dots x_{i+k-1})\).
                </p>


        <h4 style="font-size:18px;"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                <font face="arial">m filters</font>: m feature extractors</h4>

        <div style="display:grid;grid-template-columns: 45% 55%;">
            <div>
                <img src="../resources/lectures/models/cnn/several_filters_read.gif"
             style="max-width:90%; margin-right:20px; margin-bottom: 20px; float:left;"/>

        <p>One filter extracts a single feature. Usually, we want many features: for this,
            we have to take several filters. Each filter reads an input text and extracts a different feature -
            look at the illustration.
            The number of filters is the number of output features you want to get. With \(m\) filters
                    instead of one, the size of
                the convolutional layer we discussed above will become \((k\cdot d)\times m\).
        </p>
            </div>
            <div>
                <img src="../resources/lectures/models/cnn/several_filters-min.png"
             style="max-width:90%; margin-left:20px; float:right;"/>

            </div>
        </div>

        <p><font face="arial"><u>This is done in parallel!</u></font>
            Note that while I show you how a CNN "reads" a text, in practice these
            computations are done in parallel.
        </p>


        <h3><u>Basics: Pooling Operation</u></h3>

                 <p>After a convolution extracted \(m\) features from each window,
        a <font face="arial">pooling</font>
            layer summarises the features in some region.
            Pooling layers are used to reduce the input dimension, and, therefore, to reduce the number
            of parameters used by the network.</p>





        <h4 style="font-size:18px;"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                <font face="arial">Max and Mean Pooling</font></h4>

        <p>
        The most popular is <font face="arial">max-pooling</font>: it takes maximum over each dimension, i.e. takes
            the maximum value of each feature.
        </p>
        <img src="../../resources/lectures/models/cnn/max_pooling-min.png"
             style="max-width:100%; margin-bottom:20px;"/>
        <p>Intuitively, each feature "fires" when it sees some pattern: a
            visual pattern in an image (line, texture, a cat's paw, etc) or
            a text pattern (e.g., a phrase).
            After a pooling operation,
        we have a vector saying which of these patterns occurred in the input.
        </p>
        <p><font face="arial">Mean-pooling</font> works similarly but computes mean over each
        feature instead of maximum.</p>

        <h4 style="font-size:18px;"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                <font face="arial">Pooling and Global Pooling</font></h4>

        <p>Similarly to convolution, <font face="arial">pooling</font> is applied to windows
        of several elements. Pooling also has the stride parameter, and the most common approach is to use
        pooling with non-overlapping windows. For this, you have to set the stride parameter the same
        as the pool size. Look at the illustration.</p>

        <center>
        <img src="../../resources/lectures/models/cnn/pooling_with_stride-min.png"
             style="max-width:70%; margin-bottom:20px;"/>
            </center>

        <p>The difference between <font face="arial">pooling</font> and
        <font face="arial">global pooling</font> is that <font face="arial">pooling</font> is applied over features
        in each window independently, while
        <font face="arial">global pooling</font> performs over the whole input.
        For texts, global pooling is often used to get a single vector representing the whole text; such
        global pooling is called <font face="arial">max-over-time pooling</font>, where the "time" axis
        goes from the first input token to the last.</p>

        <center>
        <img src="../../resources/lectures/models/cnn/pooling_vs_global-min.png"
             style="max-width:90%; margin-bottom:20px;"/>
            </center>

        <p>Intuitively, each feature "fires" when it sees some pattern: a
            visual pattern in an image (line, texture, a cat's paw, etc) or
            a text pattern (e.g., a phrase).
            After a pooling operation,
        we have a vector saying which of these patterns occurred in the input.
        </p>

        <h3><u>Convolutional Neural Networks for Text Classification</u></h3>

        <p>Now, when we understand how the convolution and pooling work, let's come to modeling modifications.
            First, let us recall what we need:</p>
        <div class="green_left_thought" style="font-size:18px;">
                <p class="data_text">
                    We need a model
                    that can produce a <strong>fixed-sized</strong> vector
                    for inputs of <strong>different</strong> lengths.
                </p>
            </div>

        <p>Therefore, we need to construct a convolutional model that represents a text as a single vector.</p>


        <p>
        The basic convolutional model for text classification is shown in the figure.
        It is almost the same as we saw before: the only thing that's changed is that we specified the type of
            pooling used. Specifically, after the convolution, we use <font face="arial">global-over-time pooling</font>.
            This is the key operation: it allows to compress a text into a single vector.
            The model itself can be different, but at some point, it has to use the global pooling to
            compress input in a single vector.
        </p>


        <center>
        <img src="../resources/lectures/text_clf/neural/cnn/model_basic-min.png"
             style="max-width:80%; margin-bottom:20px;"/>
        </center>

        <h4 style="font-size:18px;" id="Several_Convolutions_Different_Kernels"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                <font face="arial">Several Convolutions with Different Kernel Sizes</font></h4>

        <p>Instead of picking one kernel size for your convolution, you can use several convolutions
        with different kernel sizes. The recipe is simple: apply each convolution to the data,
        add non-linearity and global pooling after each of them,
            then concatenate the results (on the illustration, non-linearity is omitted for simplicity).
            This is how you get vector representation of the data which
            is used for classification.
        </p>

        <center>
        <img src="../resources/lectures/text_clf/neural/cnn/several_kernel_sizes-min.png"
             style="max-width:100%; margin-bottom:20px;"/>
        </center>
        <p>This idea was used, among others, in the paper
        <a href="https://www.aclweb.org/anthology/D14-1181.pdf" target="_blank">
            Convolutional Neural Networks for Sentence Classification</a> and many follow-ups.
        </p>


        <h4 style="font-size:18px;"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                <font face="arial">Stack Several Blocks Convolution+Pooling</font></h4>


        <p>Instead of one layer, you can stack several blocks convolution+pooling on top of each other.
            After several blocks, you can apply another convolution, but with global pooling this time.
            Remember: you have to get a single fixed-sized vector - for this, you need global pooling.
        </p>
        <p>Such multi-layered convolutions can be useful when your texts are very long; for example,
        if your model is character-level (as opposed to word-level).</p>

        <center>
        <img src="../resources/lectures/text_clf/neural/cnn/several_blocks-min.png"
             style="max-width:90%; margin-bottom:20px;"/>
        </center>
        <p>This idea was used, among others, in the paper
        <a href="https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf" target="_blank">
            Character-level Convolutional Networks for Text Classification</a>.
        </p>


    </div>

</div>

<br>
<div id="multi_label">
    <h1>Multi-Label Classification</h1>

    <div class="carousel" data-flickity='{ "imagesLoaded": true, "percentPosition": true, "wrapAround": true }'
     style="width:50%; margin-bottom:30px; margin-left:10px; float:right;">
              <div class="carousel-cell" style="width:100%"><center>
                    <img width=70% src="../resources/lectures/text_clf/intro/example_twitter-min.png"/></center>
                  <p style="text-align:center;margin-left:30px;margin-right:30px;">
                      Multi-label classification:<br> many labels, several can be correct</p>
              </div>
              <div class="carousel-cell" style="width:100%"><center>
                    <img width=70% src="../resources/lectures/text_clf/intro/example_user-min.png"/></center>
                  <p style="text-align:center;margin-left:30px;margin-right:30px;">
                      Multi-label classification:<br> many labels, several can be correct</p>
              </div>
      </div>

        <p><font face="arial">Multi-label</font> classification is different from
        the <font face="arial">single-label</font> problems we discussed before in that each input can have several
        correct labels. For example, a twit can have several hashtags, a user can have several
        topics of interest, etc.
    </p>

    <p>For a multi-label problem, we need to change two things in the single-label pipeline we discussed before:
    </p>
    <ul>
        <li><font face="arial">model</font> (how we evaluate class probabilities);</li>
        <li><font face="arial">loss function</font>.</li>
    </ul>


    <h2>Model: Softmax → Element-wise Sigmoid</h2>
    <p>After the last linear layer,
    we have \(K\) values corresponding to the \(K\) classes - these are the values we
        have to convert to class probabilities.
    </p>
    <p>
        For single-label problems, we used softmax: it converts \(K\) values into a probability distribution,
        i.e. the sum of all probabilities is 1. It means that the classes
        share the same probability mass:
        if the probability of one class is high, other classes can not have large probability
        (<span class="data_text"><font color="#888"><u>Lena</u>: Once again, imagine a bunch of kittens eating from the same bowl:
    one kitten always eats at the expense of the others</font></span>).
    </p>


    <img width=55% src="../resources/lectures/text_clf/multi_label/model-min.png" style="float:right; margin-left:20px;"/>

    <p>For multi-label problems, we convert each of the \(K\) values into a probability
        of the corresponding class independently from the others.
        Specifically, we apply the sigmoid function \(\sigma(x)=\frac{1}{1+e^{-x}}\)
        to each of the \(K\) values.
    </p>
    <p>Intuitively, we can think of this as having
        \(K\) independent binary classifiers that use the same text representation.
    </p>


    <h2>Loss Function: Binary Cross-Entropy for Each Class</h2>

    <p>Loss function changes to enable multiple labels: for each class, we use the binary cross-entropy loss.
    Look at the illustration.</p>

    <center>
    <img width=100% src="../resources/lectures/text_clf/multi_label/loss-min.png" style="margin-left:20px;"/>
    </center>

</div>

    <br>
    <div id="practical_tips">
    <h1>Practical Tips</h1>

        <h2>Word Embeddings: how to deal with them?</h2>

            <img src="../resources/lectures/text_clf/practical_tips/embeddings_what_to_do-min.png"
             style="max-width:60%; margin-bottom:20px;margin-left:20px;float:right;"/>
            <p>Input for a network is represented by word embeddings. You have three options
                how to get these embeddings for your model:</p>
                <ul>
                    <li>train from scratch as part of your model,</li>
                    <li>take pretrained (Word2Vec, GloVe, etc) and fix them (use them as static vectors),</li>
                    <li>initialize with pretrained embeddings and train them with the network ("fine-tune").</li>
                </ul>

        <p>Let's think about these options by looking at the data a model can use.
            Training data for classification is labeled and task-specific,
            but labeled data is usually hard to get.
            Therefore, this corpus is likely to be not huge (at the very least), or not diverse, or both.
            On the contrary, training data for word embeddings are not labeled - plain texts are enough.
            Therefore, these datasets
            can be huge and diverse - a lot to learn from.
        </p>
        <img src="../resources/lectures/text_clf/practical_tips/data_types-min.png"
             style="max-width:100%; margin-bottom:20px;"/>

        <p>Now let us think about what a model will know depending on what we do with the embeddings.
            If the embeddings are trained from scratch, the model will "know" only
            the classification data - this may not be enough to learn relationships between words well.
            But if we use pretrained embeddings, they (and, therefore, the whole model)
            will know a huge corpus - they will learn a lot about the world.
            To adapt these embeddings to your task-specific data,
            you can fine-tune these embeddings by training them with the
                whole network - this can bring gains in the performance (not huge though).
        </p>

        <img src="../resources/lectures/text_clf/practical_tips/embs_what_to_do_with_data-min.png"
             style="max-width:100%; margin-bottom:20px;"/>
        <p>When we use pretrained embeddings, this is an example of
            <font face="arial">transfer learning</font>: through the embeddings, we "transfer" the knowledge
            of their training data to our task-specific model.
            We will learn more about transfer learning later in the course.
        </p>


    <div class="card_with_ico">
    <img class="ico" src="../resources/lectures/ico/bulb_empty.png"/>
    <div class="text_box_yellow">
    <p class="data_text">
        <strong>Fine-tune pretrained embeddings or not?</strong> Before training models, you can first think
        why fine-tuning can be useful, and which types of examples can benefit from it.<br>
        Learn more from <a href="#research_finetune_emb_intuition">this exercise</a>
    in the <a href="#research_thinking">Research Thinking</a> section. </p>
    </div>
    </div>

    <div class="card_with_ico">
    <img class="ico" width="40" src="../resources/lectures/ico/book_empty.png"/>
    <div class="text_box_pink">
        <p class="data_text">
            For more details and the experiments with different settings for word embeddings, look
                <a href="#paper_cnn_word_emb_strategies">at this paper summary.</a>
        </p>
    </div>
    </div>



    <h2 id="data_augmentation">Data Augmentation: Get More Data for Free</h2>

        <p><font face="arial">Data augmentation</font> alters your dataset in different ways
        to get alternative versions of the same training example. Data augmentation
            can increase
        </p>
        <ul>
            <li><font face="arial">the amount of data</font><br>
                The quality of your model depends a lot on your data. For deep learning models, having
                large datasets is very (very!) important.
            </li>
            <li><font face="arial">diversity of data</font><br>
                By giving different versions of training examples, you teach a model to be
                more robust to real-world data which can be of lower quality or simply
                a bit different from your training data. With augmented data, a model is less likely to overfit
                to specific types of training examples and will rely more on general patterns.
            </li>
        </ul>

        <p>Data augmentation for images can be done easily: look at the examples below.
            The standard augmentations include flipping an image, geometrical transformations (e.g. rotation and stretching
            along some direction), covering parts of an image with different patches.
        </p>
        <img src="../../resources/lectures/text_clf/practical_tips/image_augmentations-min.png"
         style="max-width:100%; margin-bottom:20px;"/>

        <h3><u>How can we do something similar for texts?</u></h3>

        <h4 style="font-size:18px;"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                <font face="arial">word dropout</font> - the most simple and popular</h4>

        <p>Word dropout is the simplest regularization: for each example, you choose some words randomly
        (say, each word is chosen with probability 10%) and replace the chosen words with either the special token
        <font class="data_text"><strong>UNK</strong></font> or with a random token from the vocabulary.
        </p>
        <img src="../../resources/lectures/text_clf/practical_tips/word_dropout-min.png"
         style="max-width:100%; margin-bottom:20px;"/>
        <p>The motivation here is simple: we teach a model not to over-rely on individual tokens,
            but take into consideration the context of the whole text. For example, here we masked
            <font class="data_text"><strong>great</strong></font>, and a model has to understand
            the sentiment based on other words.
        </p>

        <div style="display:grid;grid-template-columns: 65% 35%;">
            <div><p class="data_text"><u>Note:</u> For images, this corresponds to masking out some areas.
            By masking out an area of an image, we also want a model not to over-rely on local
                features and to make use of a more global context.
            </p></div>
            <div><img src="../../resources/lectures/text_clf/practical_tips/cat_dropout-min.png"
         style="max-width:90%; margin-left:30px;float:right;"/></div>
        </div>



        <h4 style="font-size:18px;"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                <font face="arial">use external resources (e.g., thesaurus)</font> - a bit more complicated</h4>
        <p>A bit more complicated approach is to replace words or phrases with their synonyms.
        The tricky part is getting these synonyms: you need external resources, and they are rarely available
        for languages other than English (for English, you can use e.g. WordNet). Another problem
            is that for languages with rich morphology (e.g., Russian) you are likely to
            violate the grammatical agreement.
        </p>
        <img src="../../resources/lectures/text_clf/practical_tips/thesaurus-min.png"
         style="max-width:100%; margin-bottom:20px;"/>


        <h4 style="font-size:18px;"><span style="margin-right:15px;font-size:14px;">&#8226;</span>
                <font face="arial">use separate models</font> - even more complicated</h4>
        <p>An even more complicated method is to paraphrase the whole sentences using external models.
        A popular paraphrasing method is to translate a sentence to some language and back. We will learn
            how to train a translation model a bit later (in the <a href="../nlp_course.html#preview_seq2seq_attn" target="_blank">Seq2seq and Attention</a> lecture),
            but for now, you can use industrial systems, e.g.
            <a href="https://translate.yandex.com" target="_blank">Yandex Translate</a>,
            <a href="https://translate.google.com" target="_blank">Google Translate</a>, etc.
            (<span class="data_text"><font color="#888">Lena: Obviously, personally I'm
                biased towards Yandex :) </font></span>)
            Note that you can combine translation systems and languages to get several paraphrases.
        </p>

        <img src="../../resources/lectures/text_clf/practical_tips/translations-min.png"
         style="max-width:100%; margin-bottom:20px;"/>


        <div style="display:grid;grid-template-columns: 65% 35%;">
            <div><p class="data_text"><u>Note:</u> For images, the last two techniques correspond to
            geometric transformations: we want to change text, but to preserve the meaning.
                This is different from word dropout, where some parts are lost completely.
            </p></div>
            <div><img src="../../resources/lectures/text_clf/practical_tips/cat_transformation-min.png"
         style="max-width:90%; margin-left:30px;float:right;"/></div>
        </div>

 </div>



<br><br><br>

<div id="analysis_interpretability">
        <img height="40" src="../resources/lectures/ico/analysis_empty.png"
             style="float:left; padding-right:20px; "/>
        <h1>Analysis and Interpretability</h1>

    <h2>What do Convolutions Learn? Analyzing Convolutional Filters</h2>
    <h3><u>Convolutions in Computer Vision: Visual Patterns</u></h3>
    <p>Convolutions were originally developed for images, and there's already a pretty good understanding
    of what the filters capture and how filters from different layers from a hierarchy.
    While lower layers capture simple visual patterns such as lines or circles, final
        layers can capture the whole pictures, animals, people, etc.
    </p>

    <p style="text-align: center;  display: block; margin-left:25px;
         margin-top:-10px; max-width:100%;">
            <img width=90% src="../resources/lectures/text_clf/analysis/filters_in_images-min.png" alt="" /><br />
            <span style="font-size: small;">
                Examples of patterns captured by convolution filters for images.
                The examples are from
        <a href="https://distill.pub/2019/activation-atlas/" target="_blank">
            Activation Atlas from distill.pub</a>.</span></p>

    <h3><u>What About Convolutions in Texts?</u></h3>

    <p class="data_text">This part is based on the paper
        <a href="https://arxiv.org/pdf/1809.08037.pdf" target="_blank">Understanding Convolutional Neural Networks for Text Classification</a>.</p>


    <p>For images, filters capture local visual patterns which are important for classification.
        For text, such local patterns are word n-grams.
        The main findings on how CNNs work for texts are:</p>
    <ul>
        <li><font face="arial">convolving filters are used as ngram detectors</font><br>
            Each filter specializes in one or several families of closely-related ngrams.
            Filters are not homogeneous, i.e. a single filter can, and often does,
            detect multiple distinctly different families of ngrams.
        </li>
        <li><font face="arial">max-pooling induces a thresholding behavior</font><br>
            Values below a given threshold are ignored when (i.e. irrelevant to) making a prediction.
            For example, <a href="https://arxiv.org/pdf/1809.08037.pdf" target="    ">this paper</a>
            shows that 40% of the pooled ngrams on average can be dropped with no loss of performance.
        </li>
    </ul>

    <img width=55% src="../resources/lectures/text_clf/analysis/look_at_filter-min.png" alt=""
         style="float:right; margin-left:20px;" />
    <p>The simplest way to understand what a network captures is to look which patterns activate its neurons.
        For convolutions, we pick a filter and find those n-grams which activate this filter most.
    </p>

    <p>Below are examples of the top-1 n-gram for several filters. For one of them, we also
    show other n-grams which lead to high activation of this filter - you can see that the n-grams
    have a very similar meaning.</p>


    <center>
    <img width=75% src="../resources/lectures/text_clf/analysis/top_ngrams-min.png" alt=""
    style="margin-bottom:20px;"/>
        </center>

    <p>For more details, look at the paper
        <a href="https://arxiv.org/pdf/1809.08037.pdf" target="_blank">
            Understanding Convolutional Neural Networks for Text Classification</a>.
    </p>


    <h2>How About RNN CLassifiers?</h2>

        <div class="card_with_ico">
        <img class="ico" width="40" src="../resources/lectures/ico/book_empty.png"/>
        <div class="text_box_pink">
            <p class="data_text">

                How RNNs trained for classification process text?

                    Learn  <a href="#papers_rnn_analysis">here</a>.
            </p>
        </div>
    </div>


</div>




<br><br><br>
<div id="research_thinking">
        <img height="40" src="../resources/lectures/ico/bulb_empty.png"
             style="float:left; padding-right:10px; margin-top:-20px;"/>
        <h1 style="margin-left:10px; margin-right:20px; float: left; margin-top:-20px">Research Thinking</h1>
        <hr color="#fced95" style="height:5px">
<br><br>


<fieldset style="border: 1px solid #f0e4a5;
    border-radius: 5px;">
            <legend><p class="data_text"><strong>How to</strong></p></legend>
            <ul class="data_text">
                <li>Read the  short description at the beginning - this is our starting point,
        something known.</li>
                <li>Read a question and think: for a minute, a day, a week, ... -
        give yourself some time! Even if you are not thinking about it constantly,
        something can still come to mind.</li>
                <li>Look at the possible answers - previous attempts to answer/solve this problem.<br>
                    <u>Important:</u>
                    You are <strong>not</strong> supposed to come up with
                    something exactly like here - remember, each paper usually takes the authors several
                    months of work. It's a habit of thinking about these things that counts!
                    All the rest a scientist needs is time: to try-fail-think
                    until it works.</li>
            </ul>

            <p class="data_text">It's well-known that you will learn something easier if you are not just answered right away,
            but if you think about it first. Even if you don't want to be a researcher, this is still a good way
            to learn things!</p>
</fieldset>


        <br><br>


        <!--##################################################-->
        <!--##################################################-->
        <!--##################################################-->

        <div class="research_circle" style="float:left;"></div>
        <h2 style="margin-top:-10px; float: left; padding-left:10px; padding-right:10px; color:#786702">
            Classical Approaches</h2>
        <div class="box_yellow_left">

        <!--##################################################-->
        <div class="researchCard" id="thumbnail_research" >
            <div class="researchIntro" id="research_improve_bayes">

              <div class="cardContent">

                  <div class="research_title">
                      Improve Naive Bayes
                  </div>

                <hr color="#dedeca" style="margin:5px;">
                  The simplest Naive Bayes implementation uses tokens as features.
                  However, this is not always good: completely
                  different texts can have the same features.

              </div>
                <div>
                      <img src="../resources/lectures/text_clf/research/bayes_good_bad-min.png"
                           alt="" style="margin-top:20px;" class="center"/>

                </div>

             </div>
            <hr color="#dedeca" style="margin:5px">
            <div class="cardContent">

                <span class="research_question">?</span>
                In the example above we see the main problem of Naive Bayes:
                it knows nothing about context. Of course, we can not remove the "naive" assumptions
                (otherwise, it won't be Naive Bayes anymore). But can we improve the feature extraction part?
                <br>
                <details>
                    <summary  class="research_summary">
                       Possible answers</summary>
                    <br>

                    <h3>Idea: Add Frequent N-Grams to the Features!</h3>

                    <center>
                    <img src="../resources/lectures/text_clf/research/bayes_add_ngrams-min.png"
                           alt="" style="margin-bottom:20px; width: 80%"/>
                    </center>

                    <p>Instead of using only words as features, we can also use word n-grams.
                    Since using all n-grams would be inefficient, we can add only the frequent ones. This can
                        fix some examples with simple negation as the one shown above.
                    </p>
                </details>

                <br>
                <span class="research_question">?</span>
                What other types of features you can come up with?
                <br>
                <details>
                    <summary  class="research_summary">
                       Possible answers</summary>
                    <br>

                    <p>Note that Naive Bayes can use any categorical features - you can do anything
                    as long as you can compute the counts for probabilities. For example,</p>
                    <ul>
                        <li><font face="arial">text length</font><br>
                            Who knows - maybe positive reviews are longer than negative?
                        Don't forget to categorize it, e.g., 1-20 tokens
                            correspond to one feature, 21-30 tokens - to another, etc.
                        </li>
                        <li><font face="arial">token frequency</font><br>
                            It may be worth checking - positive or negative reviews use more
                            peculiar words?
                            You can come up with some characteristics of tokens frequency:
                            minimum of maximum, average, etc. But again - you have to categorize it!
                        </li>
                        <li><font face="arial">syntactical features</font>
                            (if you don't know what it is yet, skip this)<br>
                            Dependency tree depth (maximum/minimum/average) - this can be a proxy of text complexity.
                        </li>
                        <li><font face="arial">anything else you can come up with<br></font>
                        Just try :)
                    </ul>
                </details>


                <br>
                <span class="research_question">?</span>
                Are all words equally needed for classification?
                If not, how can we modify the method?
                <details>
                    <summary class="research_summary">
                        Possible answers</summary>

                    <br>
                    <h3>Idea: Do Not Use Unimportant Words</h3>
                    <img src="../resources/lectures/text_clf/research/bayes_stop_words-min.png"
                           alt="" style="margin-left:20px; float:right; width: 60%"/>

                    <p>If you know which words definitely do not influence
                    class probability, you can remove them from the features! For example, we can remove stop-words:
                        determiners, prepositions, etc.
                    </p>

                    <p>Note: you need to be really careful  - don't remove something useful!</p>
                </details>

            </div>
        </div>

        <!--##################################################-->
        </div>
        <div class="research_circle" style="float:left;"></div>
        <!--##################################################-->
        <!--##################################################-->
        <!--##################################################-->

    <br><br>

        <!--##################################################-->
        <!--##################################################-->
        <!--##################################################-->

        <div class="research_circle" style="float:left;"></div>
        <h2 style="margin-top:-10px; float: left; padding-left:10px; padding-right:10px; color:#786702">
            Neural Approaches</h2>
        <div class="box_yellow_left">

        <!--##################################################-->
        <div class="researchCard" id="thumbnail_research" >
            <div class="researchIntro" id="research_finetune_emb_intuition">

              <div class="cardContent">

                  <div class="research_title">
                      Fine-tuning embeddings: Why and when can this help?
                  </div>

                <hr color="#dedeca" style="margin:5px;">
                  Before training models, you can first think why fine-tuning can be useful,
                  and which types of examples can benefit from it.
                  Remember how embeddings are trained: words that are used similarly in texts
                  have very close embeddings.
                  Therefore, sometimes antonyms are closest to each other, e.g.
                  <font class="data_text"><strong>descent</strong></font> and
                  <font class="data_text"><strong>ascent</strong></font>.


              </div>
                <div>
                    <!--<div class="research_tag">neural</div>-->

                      <img src="../resources/lectures/text_clf/research/antonyms_close-min.png"
                           alt="" style="margin-top:20px;width:85%" class="center"/>

                </div>

             </div>
            <hr color="#dedeca" style="margin:5px">
            <div class="cardContent">

                <span class="research_question">?</span>
                Imagine we want to use embeddings for sentiment classification.
                Can you find examples of antonyms such that if their embeddings are very close,
                it would hurt sentiment classification?
                If you can, it means that might be better to fine-tune!
                <br>
                <details>
                    <summary  class="research_summary">
                       Possible answers</summary>
                    <br>

                    <h3>Without fine-tuning, closest to <font class="data_text"><strong>bad</strong></font> is
                    <font class="data_text"><strong>good</strong></font>!</h3>

                    <p>The figure shows closest neighbors of Word2Vec embeddings before and after fine-tuning
                        (examples are taken from <a href="https://www.aclweb.org/anthology/D14-1181.pdf" target="_blank">this paper</a>).
                    </p>
                    <center>
                    <img src="../resources/lectures/text_clf/research/good_bad_neighbors-min.png"
                           alt="" style="margin-bottom:20px; width: 90%"/>
                    </center>

                    <p>Without fine-tuning, closest to <font class="data_text"><strong>bad</strong></font> is
                    <font class="data_text"><strong>good</strong></font>! Without fine-tuning,
                        it would be very hard for a model to separate positive and negative using these embeddings.
                        This is only one example of antonyms with close embeddings which can hurt sentiment classification.
                    </p>

                    <p>Fine-tuning can also help to improve understanding of tokens
                        such as <font class="data_text"><strong>n't</strong></font>: rare
                        in the corpus
                        word embeddings were trained on, but not rare in the corpus we care about.
                        More generally, if your task-specific <font face="arial">domain</font>
                        is different from the
                        word embeddings training data, fine-tuning is a good idea.
                    </p>
                </details>



            </div>
        </div>

        <!--##################################################-->
        </div>
        <div class="research_circle" style="float:left;"></div>
        <!--##################################################-->
        <!--##################################################-->
        <!--##################################################-->


    <br><br>

        <div style="border: 0px solid #ccc;border-radius:15px;margin: 10px; padding: 4px;
 background-color: #f5f5f5;margin-top:-20px;">
    <div style="display: grid; grid-template-columns: 75% 25%; margin:10px;">
                <div>
                <div style="margin-top:20px;">
                    <p style="margin:30px; font-size:30px;">Here will be more exercises!</p>

                    <p style="margin:30px;">This part will be expanding from time to time.</p>
                </div>
                    </div>
                <div>
                    <center>
                    <img src="../../resources/lectures/main/preview/pusheen_draws_on_white-min.png"
                       style="width:80%; padding-top:20px; padding-bottom:20px;border-radius:50%">
                    </center>
                </div>
            </div>

        </div>





</div>



<br><br><br><br>
    <!--#########################################################################################################-->
    <!--#########################################################################################################-->
    <!--#########################################################################################################-->

<div id="related_papers">
        <img height="40" src="../resources/lectures/ico/book_empty.png"
             style="float:left; padding-right:10px; margin-top:-20px;"/>
        <h1 style="margin-left:10px; margin-right:20px; float: left; margin-top:-20px">Related Papers</h1>
        <hr color="#facae9" style="height:5px">

        <br><br>




<fieldset style="border: 1px solid #dec8d6;
    border-radius: 5px;">
            <legend><p class="data_text"><strong>How to</strong></p></legend>
            <ul class="data_text">
            <li><u>High-level</u>: look at key results in short summaries -
                get an idea of what's going on in the field.</li>
            <li><u>A bit deeper</u>: for topics which interest you more,
                read longer summaries with illustrations and explanations.
                Take a walk through the authors' reasoning steps and key observations. </li>
            <li><u>In depth</u>: read the papers you liked. Now, when you got the main idea, this
            is going to be easier!</li>
            </ul>
</fieldset>

        <br><br>

        <p class="data_text" style="font-size:24px;color:#7a3160">What's inside:</p>
        <ul class="data_text" style="font-size:20px;color:#7a3160">
            <li><a href="#papers_cnn_classics">Convolutions for Classification: Classics</a></li>
            <li><a href="#papers_rnn_analysis">Analyzing RNNs for Sentiment Classification</a></li>

            <li>... to be updated</li>
        </ul>


        <br><br>


        <!--##################################################-->
        <!--##################################################-->
        <!--##################################################-->

        <div class="paper_circle" style="float:left;"></div>
        <h2 style="margin-top:-10px; float: left; padding-left:10px; padding-right:10px; color:#7a3160">
            Convolutions for Classification: Classics</h2>
        <div class="box_pink_left" id="papers_cnn_classics">


        <!--##################################################-->
        <div class="paperCard" id="thumbnail_paper" >
            <div class="paperIntro" id="paper_cnn_word_emb_strategies">

              <div class="cardContent">

                  <div class="paper_title">
                      <a href="https://www.aclweb.org/anthology/D14-1181.pdf" target="_blank" class="links">
                      Convolutional Neural Networks for Sentence Classification
                      </a>
                  </div>

                <div class="paper_authors">
                    Yoon Kim
                </div>
                <hr color="#f2e4ee" style="margin:5px;">
                  <p>Even a very simple CNN with one layer on top of word embeddings
                      shows very good performance (without features requiring some external knowledge!).
                      The paper also shows
                      the importance of using pretrained embeddings (and not training from scratch)
                      and gains from fine-tuning.

                  </p>
              </div>

                <div>
                    <div class="conf_name">EMNLP 2014</div>
                  <a href="https://www.aclweb.org/anthology/D14-1181.pdf" target="_blank">
                      <img src="../resources/lectures/text_clf/papers/conv_cnn_classic-min.png"
                           alt="" style="margin-top:30px;" class="center"/>
                  </a>
                </div>

             </div>
            <hr color="#f2e4ee" style="margin:5px">
            <div class="cardContent">
                <details>
                    <summary style="margin-left:10px;">More details</summary>
                    <br>
                    <p>The CNN model is shown in the figure: on top of embeddings, it has three convolutions
                        with max-over-time pooling (in parallel). The results are concatenated and used for classification.
                        This is a very simple model
                        <a href="#Several_Convolutions_Different_Kernels">we discussed
                        earlier</a>.
                     </p>
                    <p>The paper has a lot of results (comparison with 14 baselines!), but
                    here I'll mention only the ones comparing the same CNN model with different
                    strategies of obtaining word embeddings.
                    </p>

                    <h2>Embeddings: Random vs Pretrained (Word2Vec)</h2>
                    <p>In the table below
                        <font face="arial">random</font> experiment -
                        embeddings are randomly initialized and trained with the model,
                        <font face="arial">pretrained</font> - the embeddings are initialized
                        with Word2Vec and fixed (not trained with the model).</p>
                    <img src="../resources/lectures/text_clf/papers/random_vs_pretrained-min.png"
                           alt="" style="margin-left:20px;width:60%;float:right;"/>
                    <p>We see that using pretrained embeddings is better by several percentage points!
                    This happens because in the first case the embeddings see only the classification training data,
                        which is usually not much. But trained embeddings
                        saw a lot of other data - they know a lot more about the world and relationships between words.
                    </p>
                    <p><u>Important!</u> This is called <font face="arial">transfer learning</font> -
                        by using trained embeddings,
                        you "transfer" knowledge contained in the embeddings training data
                        to your model. We will learn more about this later in the course.
                    </p>


                    <h2>Pretrained Embeddings: Fixed vs Fine-tuned</h2>
                    <p>In the table below
                        <font face="arial">fixed</font> experiment -
                        the embeddings are initialized
                        with Word2Vec and fixed (not trained with the model),
                        <font face="arial">fine-tuned</font> - the embeddings are initialized
                        with Word2Vec and then trained with the classification model.</p>
                    <img src="../resources/lectures/text_clf/papers/fixed_vs_finetuned-min.png"
                           alt="" style="margin-left:20px;width:60%;float:right;"/>
                    <p>We see that if we fine-tune pretrained embeddings for the specific task,
                        we are likely to get further improvement. But you need to be careful -
                        embeddings can "forget" what they learned before.
                    </p>

                    <h2>More in the paper</h2>

                    <ul>
                        <li><font face="arial">comparison with lots of baselines</font><br>
                        This simple model performs quite well compared to lots of baselines (including SVMs we discussed above).
                        </li>
                        <li><font face="arial">both fixed and fine-tuned embeddings</font><br>
                        The paper proposes a way to use both fixed embeddings and fine-tuned:
                            it duplicates the embedding layer and keeps one of them fixed while trains the other.
                            The results are mixed - look in the paper!
                        </li>
                    </ul>

                </details>
            </div>
        </div>

        <!--##################################################-->


        <!--##################################################-->
        <div class="paperCard" id="thumbnail_paper" >
            <div class="paperIntro">

              <div class="cardContent">

                  <div class="paper_title">
                      <a href="https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf" target="_blank" class="links">
                      Character-level Convolutional Networks for Text Classification
                      </a>

                  </div>

                <div class="paper_authors">
                    Xiang Zhang, Junbo Zhao, Yann LeCun
                </div>
                <hr color="#f2e4ee" style="margin:5px;">
                <p>This is the first paper showing that CNNs only on characters can do quite well. This is interesting:
                    classification can be done without any external knowledge, even without
                    text segmentation into
                    words! An important point is that character-level CNNs can do better than classical approaches
                    only for large datasets.
                </p>

              </div>

                <div>
                    <div class="conf_name">NeurIPS 2015</div>
                  <a href="https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf" target="_blank">
                      <img src="../resources/lectures/text_clf/papers/char_cnn-min.png"
                           alt="" style="margin-top:30px;width:85%;" class="center"/>
                  </a>
                </div>

             </div>
            <hr color="#f2e4ee" style="margin:5px">
            <div class="cardContent">
                <details>
                    <summary style="margin-left:10px;">More details</summary>

                    <br>
                    <p>The CNN model is shown in the figure above. It has 6 convolution+pooling blocks
                    (note that the last pooling is global) followed by 3 linear layers (with non-linearities
                        after each convolution and linear layers).</p>

                    <h2>Character-level CNN is better for large datasets</h2>

                    <p>The figure below shows the relative errors with respect to comparison models.
                        Each of these plots is computed by taking the difference between errors on
                        a comparison model and the character-level CNN model,
                        then divided by the comparison model error.</p>
                    <p>Here we show only 2 of the comparisons: with a classical approach
                        (n-grams tf-idf) and a word-level CNN. The results show that
                    for large datasets, character-level CNN performs better.</p>

                    <img width=90% src="../resources/lectures/text_clf/papers/char_cnn_exps-min.png"
                           alt="" style="margin-bottom:15px;"/>

                    <h2>Other Results</h2>
                    <ul>
                        <li><font face="arial">choice of alphabet matters</font><br>
                            You can take either only lowercased data, or distinguish
                            between uppercase and lowercase - the results can be different!
                        </li>
                        <li><font face="arial">char-CNNs work better for user-generated data.</font><br>
                            Compared to word-level models, character-level ones may be better suitable for
                            raw user data with misspellings.</li>
                    </ul>
                    <p>For more details, look at
                        <a href="https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf" target="_blank">
                            the paper</a>!</p>
                </details>
            </div>
        </div>

        <!--##################################################-->

        </div>
        <div class="paper_circle" style="float:left;"></div>

        <!--##################################################-->
        <!--##################################################-->
        <!--##################################################-->


        <br><br>


        <!--##################################################-->
        <!--##################################################-->
        <!--##################################################-->

        <div class="paper_circle" style="float:left;"></div>
        <h2 style="margin-top:-10px; float: left; padding-left:10px; padding-right:10px; color:#7a3160">
            Analyzing RNNs for Sentiment Classification</h2>
        <div class="box_pink_left" id="papers_rnn_analysis">


        <!--##################################################-->
        <div class="paperCard" id="thumbnail_paper" >
            <div class="paperIntro">

              <div class="cardContent">

                  <div class="paper_title">
                      <a href="https://papers.nips.cc/paper/9700-reverse-engineering-recurrent-networks-for-sentiment-classification-reveals-line-attractor-dynamics.pdf" target="_blank" class="links">
                      Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics
                      </a>
                  </div>

                <div class="paper_authors">
                    Niru Maheswaranathan, Alex H. Williams, Matthew D. Golub, Surya Ganguli, David Sussillo
                </div>
                <hr color="#f2e4ee" style="margin:5px;">
                  <p>If we take an RNN trained for sentiment analysis and apply PCA to lots of its states,
                      we'll see that almost all variance is explained with only two components.
                      Moreover, when such RNN reads a text, its states move along a 1D plane in either negative
                      or positive direction depending on the word it reads.
                  </p>
              </div>

                <div>
                    <div class="conf_name">NeurIPS 2019</div>
                  <a href="https://papers.nips.cc/paper/9700-reverse-engineering-recurrent-networks-for-sentiment-classification-reveals-line-attractor-dynamics.pdf" target="_blank">
                      <img src="../resources/lectures/text_clf/papers/rnn_pca_components-min.png"
                           alt="" style="margin-top:30px;" class="center"/>
                  </a>
                </div>

             </div>
            <hr color="#f2e4ee" style="margin:5px">
            <div class="cardContent">
                <details>
                    <summary style="margin-left:10px;">More details</summary>
                    <br>
                    <p>The paper looks at four different RNN types (vanilla, LSTM, GRU, and the
                        <a href="https://arxiv.org/pdf/1611.09913.pdf" target="_blank">Update Gate RNN</a>)</a>,
                        as well as different sentiment classification datasets (IMDb movie review, Yelp review, SST-2).
                        The results are similar for all combinations.
                     </p>

                    <h2>PCA: Most of the Variance is Captured by a Few Dimensions</h2>
                    <img src="../resources/lectures/text_clf/papers/rnn_pca_explained_variance-min.png"
                           alt="" style="margin-left:20px;width:35%;float:right;"/>
                    <p>The authors took 1000 test examples, fed it to LSTM, took all states, and applied PCA.
                        Turns out, all variance is explained by only a couple of components! Note that this
                        is true only for a trained model - for untrained one, this is not the case.
                        Look at the figure.
                    </p>



                    <h2>The 1D Plane, Trajectories and Word Influences</h2>
                    <p>In the figure above (the red/green one) are the RNN
                        states projected onto the first two PCA components;
                        the states are colored according to the target label.
                        We see that the states lie along a one-dimensional plane corresponding to
                        the sentiment changing from one to another. The figure also shows examples of RNN trajectories
                        when reading a positive or a negative text. The further the model is in the text, the deeper
                        its state goes in the corresponding area.
                    </p>

                    <img src="../resources/lectures/text_clf/papers/words_move_rnn_states-min.png"
                           alt="" style="margin-left:20px;width:40%;float:right;"/>
                    <p>What is more interesting, the authors also looked at how each token influences the RNN state.
                        As expected, positive and negative words usually move the states in
                        the corresponding area, while neutral words do not have such influence.
                    </p>

                    <h2>More in the paper</h2>

                    <ul>
                        <li><font face="arial"> linear approximations of RNN dynamics</font><br>
                        The authors apply a linearization
                        procedure to obtain an approximate, but highly interpretable, description of the RNN dynamics.
                        </li>
                        <li><font face="arial">RNNs count the number of positive and negative words used</font><br>
                            With lots of math, the authors conclude that in nearly all
                            cases the key activity performed by the RNN for sentiment
                            analysis is simply counting the number of positive and negative words used.
                        </li>
                    </ul>

                </details>
            </div>
        </div>

        <!--##################################################-->



        </div>
        <div class="paper_circle" style="float:left;"></div>

        <!--##################################################-->
        <!--##################################################-->
        <!--##################################################-->


<br><br>
    <div style="border: 0px solid #ccc;border-radius:15px;margin: 10px; padding: 4px;
 background-color: #f5f5f5;margin-top:-20px;">
    <div style="display: grid; grid-template-columns: 75% 25%; margin:10px;">
                <div>
                <div style="margin-top:20px;">
                    <p style="margin:30px; font-size:30px;">Here will be more papers!</p>

                    <p style="margin:30px;">The papers will be gradually appearing.</p>
                </div>
                    </div>
                <div>
                    <center>
                    <img src="../../resources/lectures/main/preview/pusheen_reads_on_white-min.png"
                       style="width:80%; padding-top:20px; padding-bottom:20px;border-radius:50%">
                    </center>
                </div>
            </div>

        </div>
</div>




<br><br><br>
<div id="have_fun">
<img height="40" src="../resources/lectures/ico/fun_empty.png"
     style="float:left; padding-right:10px; margin-top:-20px;"/>
<h1 style="margin-left:10px; margin-right:20px; float: left; margin-top:-20px">Have Fun!</h1>
<hr color="#c8edfa" style="height:5px">
<br><br>


    <div style="border: 0px solid #ccc;border-radius:15px;margin: 10px; padding: 4px;
 background-color: #f5f5f5;margin-top:-20px;">
    <div style="display: grid; grid-template-columns: 75% 25%; margin:10px;">
                <div>
                <div style="margin-top:20px;">
                    <p style="margin:30px; font-size:30px;">Coming soon!</p>

                    <p style="margin:30px;">We are still working on this!</p>
                </div>
                    </div>
                <div>
                    <center>
                    <img src="../../resources/lectures/main/preview/typing.gif"
                       style="width:80%; padding-top:20px; padding-bottom:20px;border-radius:50%">
                    </center>
                </div>
            </div>

        </div>
</div>


</div>

</div>

