<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Blog</title>
    <meta name="description" content="Intuitive explanations for some of my papers.">

    <link rel="stylesheet" href="/css/main.css">
    <link rel="canonical" href="http://localhost:4000/posts.html">

    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>


</head>


  <!--style>
  p {
    text-align: justify;
  }
  </style-->

  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Lena Voita</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
          <a class="page-link" href="/posts.html">Blog</a>
          
        
          
          <a class="page-link" href="/papers.html">Publications</a>
          
        
          
          <a class="page-link" href="/talks.html">Talks & Service</a>
          
        
          
          <a class="page-link" href="/nlp_course.html">NLP Course | For You</a>
          
        
          
        
<!--        <a href="">
            <img src="image/UoS.ico" alt="DCS">
        </a>-->
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <style>

  #thumbnail {
    box-shadow: 0 5px 10px rgba(0,0,0,0.19), 0 3px 3px rgba(0,0,0,0.23);
  }
  #thumbnail:hover {
    box-shadow: 0 12px 24px rgba(0,0,0,0.19), 0 8px 8px rgba(0,0,0,0.23);
  }

  .fullCard {
    width: 750px;
    border: 1px solid #ccc;
    border-radius: 5px;
    margin: 10px 5px;
    padding: 4px;

  }
  .cardContent {
    padding: 10px;

  }

  .center {
    display: block;
    margin-left: auto;
    margin-right: auto;
  }

</style>


<div class="fullCard" id="thumbnail" >
    <div class="cardContent">

        <!--<h1 style="font-size:28px;">Language Modeling, Lexical Translation, Reordering</h1>-->
        <h1 style="font-size:28px;">Neurons in LLMs: Dead, N-gram, Positional</h1>

        <span style="font-size:14px;">
        This is a post for the paper
            <a href="https://arxiv.org/pdf/2309.04827.pdf" target="_blank">
                Neurons in Large Language Models: Dead, N-gram,  Positional.
            </a>
        </span>

        <a class="float-right">
            <img src="../resources/posts/ffn_neurons/suppressed_concepts-min.png" alt=""
                 style="max-width:300px; height:auto; float: right; margin-left:15px; margin-top:25px"/>
        </a>



        <br/>
        <br/>
        <span style="font-size:15px;">

            <p>With scale, LMs become more exciting but, at the same time, harder to analyze.
                We show that even with
                simple methods and a single GPU, you can do a lot! We analyze OPT models up to 66b and find that
            </p>
        <ul>
            <li>neurons inside LLMs can be:</li>
            <ul style="margin-left:30px;">
                <li><u>dead</u>, i.e. never activate on a large dataset,</li>
                <li><u>n-gram</u> detectors that explicitly remove information about current input token;</li>
                <li><u>positional</u>, i.e. encode "where" regardless of "what" and question the key-value memory view of FFNs;</li>
            </ul>

            <li>with scale, models have more dead neurons and token detectors and are less focused on absolute position.
            </li>
        </ul>
        </span>

        <a class="pull-right" href="/posts/neurons_in_llms_dead_ngram_positional.html" onMouseOver="document.readmore8.src='../resources/posts/buttons/button_read_more_push-min.png';" onMouseOut="document.readmore8.src='../resources/posts/buttons/button_read_more-min.png';">
        <img src="../resources/posts/buttons/button_read_more-min.png" name="readmore8" width=120px class="pull-right"></a>
        <a class="pull-right" href="https://arxiv.org/pdf/2309.04827.pdf" onMouseOver="document.readpaper8.src='../resources/posts/buttons/button_read_paper_push-min.png';" onMouseOut="document.readpaper8.src='../resources/posts/buttons/button_read_paper-min.png';">
        <img src="../resources/posts/buttons/button_read_paper-min.png" name="readpaper8" width=120px class="pull-right"></a>


        <span style="font-size:15px; text-align: right; float: right; color:gray">September 2023</span>

    </div>
</div>


<!-- ################################################################################### -->


<div class="fullCard" id="thumbnail" >
    <div class="cardContent">

        <!--<h1 style="font-size:28px;">Language Modeling, Lexical Translation, Reordering</h1>-->
        <h1 style="font-size:28px;">NMT Training Process though the Lens of SMT</h1>

        <span style="font-size:14px;">
        This is a post for the EMNLP 2021 paper
            <a href="https://arxiv.org/abs/2109.01396" target="_blank">
                Language Modeling, Lexical Translation, Reordering:
                The Training Process of NMT through the Lens of Classical SMT.
            </a>
        </span>

        <a class="float-right">
            <img src="../resources/posts/nmt_training/morda-min.png" alt=""
                 style="max-width:300px; height:auto; float: right; margin-left:15px; margin-top:25px"/>
        </a>



        <br/>
        <br/>
        <span style="font-size:15px;">

            <p>In SMT, model competences are modelled with distinct models.
                In NMT, the whole translation task is modelled
                with a single neural network.
                How and when does NMT get to learn all the competences? We show that</p>
        <ul>
            <li>during training, NMT undergoes three different stages:</li>
            <ul style="margin-left:30px;">
                <li>target-side language modeling,</li>
                <li>learning how to use source and approaching word-by-word translation,</li>
                <li>refining translations, visible by increasingly complex reorderings
                    but not visible by e.g. BLEU;</li>
            </ul>

            <li>not only this is fun, but it can also help in practice! For example, in settings where
                data complexity matters, such as non-autoregressive NMT.
            </li>
        </ul>
        </span>

        <a class="pull-right" href="/posts/nmt_training_through_smt_lens.html" onMouseOver="document.readmore7.src='../resources/posts/buttons/button_read_more_push-min.png';" onMouseOut="document.readmore7.src='../resources/posts/buttons/button_read_more-min.png';">
        <img src="../resources/posts/buttons/button_read_more-min.png" name="readmore7" width=120px class="pull-right"></a>
        <a class="pull-right" href="https://arxiv.org/abs/2109.01396" onMouseOver="document.readpaper7.src='../resources/posts/buttons/button_read_paper_push-min.png';" onMouseOut="document.readpaper7.src='../resources/posts/buttons/button_read_paper-min.png';">
        <img src="../resources/posts/buttons/button_read_paper-min.png" name="readpaper7" width=120px class="pull-right"></a>


        <span style="font-size:15px; text-align: right; float: right; color:gray">September 2021</span>

    </div>
</div>


<!-- ################################################################################### -->



<div class="fullCard" id="thumbnail" >
    <div class="cardContent">

        <h1 style="font-size:28px;">Neural Machine Translation Inside Out</h1>

        <a class="float-right">
            <img src="../resources/posts/nmt_inside_out/morda_test.png" alt=""
                 style="max-width:300px; height:auto; float: right; margin-left:15px; margin-top:25px"/>
        </a>

        <span style="font-size:14px;">
        This is a blog version of my talk at the ACL 2021 workshop
            <a href="https://sites.google.com/view/repl4nlp-2021/" target="_blank">Representation
                Learning for NLP</a> (and an updated version
            of that at NAACL 2021 workshop
            <a href="https://sites.google.com/view/deelio-ws/" target="_blank">
                Deep Learning Inside Out (DeeLIO)</a>).
        </span>


        <br/>
        <br/>
        <span style="font-size:15px;">
            In the last decade, machine translation shifted from the traditional statistical approaches
            with distinct components and hand-crafted features to the end-to-end neural ones.
            We try to understand how NMT works and show that:
        <ul>
          <li>NMT model components can learn to extract features which in SMT were modelled explicitly;</li>
          <li>for NMT, we can also look at how it balances the two different types of context: the source and the prefix;</li>
          <li>NMT training consists of the stages where it focuses on competences
               mirroring three core SMT components.</li>
        </ul>
        </span>

        <a class="pull-right" href="/posts/nmt_inside_out.html" onMouseOver="document.readmore6.src='../resources/posts/buttons/button_read_more_push-min.png';" onMouseOut="document.readmore6.src='../resources/posts/buttons/button_read_more-min.png';">
        <img src="../resources/posts/buttons/button_read_more-min.png" name="readmore6" width=120px class="pull-right"></a>

        <span style="font-size:15px; text-align: right; float: right; color:gray">July 2021</span>

    </div>
</div>


<div class="fullCard" id="thumbnail" >
    <div class="cardContent">

        <h1 style="font-size:28px;">Source and Target Contributions to NMT Predictions</h1>

        <video width="300" height="auto" style="float: right; margin-left: 15px;" loop autoplay muted>
          <source src="../resources/posts/src_dst_nmt/src_dst_main.mp4" type="video/mp4">
        </video>

        <span style="font-size:14px;">
        This is a post for the ACL 2021 paper
            <a href="https://arxiv.org/pdf/2010.10907.pdf" target="_blank">
                Analyzing the Source and Target Contributions to Predictions in Neural Machine Translation.
            </a>
        </span>


        <br/>
        <br/>
        <span style="font-size:15px;">
            In NMT, the generation of a target token is based on two types of context: the source and the prefix of the target sentence.
            We show how to evaluate the relative contributions of source and target to NMT predictions and find that:
        <ul>
          <li>models suffering from exposure bias are more prone to over-relying on target history (and hence to hallucinating) than
          the ones where the exposure bias is mitigated;</li>
          <li>models trained with more data rely on the source more and do it more confidently;</li>
          <li>the training process is non-monotonic with several distinct stages.</li>
        </ul>
        </span>

        <a class="pull-right" href="/posts/source_target_contributions_to_nmt.html" onMouseOver="document.readmore5.src='../resources/posts/buttons/button_read_more_push-min.png';" onMouseOut="document.readmore5.src='../resources/posts/buttons/button_read_more-min.png';">
        <img src="../resources/posts/buttons/button_read_more-min.png" name="readmore5" width=120px class="pull-right"></a>
        <a class="pull-right" href="https://arxiv.org/pdf/2010.10907.pdf" onMouseOver="document.readpaper5.src='../resources/posts/buttons/button_read_paper_push-min.png';" onMouseOut="document.readpaper5.src='../resources/posts/buttons/button_read_paper-min.png';">
        <img src="../resources/posts/buttons/button_read_paper-min.png" name="readpaper5" width=120px class="pull-right"></a>
        <a class="pull-right" href="https://github.com/lena-voita/the-story-of-heads" onMouseOver="document.viewcode5.src='../resources/posts/buttons/button_view_code_push-min.png';" onMouseOut="document.viewcode5.src='../resources/posts/buttons/button_view_code-min.png';">
        <img src="../resources/posts/buttons/button_view_code-min.png" name="viewcode5" width=120px></a>

        <span style="font-size:15px; text-align: right; float: right; color:gray">October 2020</span>

    </div>
</div>


<!-- ################################################################################### -->

<div class="fullCard" id="thumbnail" >
    <div class="cardContent">

    <h1 style="font-size:28px;">Information-Theoretic Probing with MDL</h1>


<a class="float-right">
    <img src="../resources/posts/mdl_probes/probe_main_orange-min.png" alt=""
         style="max-width:350px; height:auto; float: right; margin-left:15px; margin-top:25px"/>
</a>


<span style="font-size:14px;">
This is a post for the EMNLP 2020 paper
    <a href="https://arxiv.org/pdf/2003.12298.pdf" target="_blank">
            Information-Theoretic Probing with Minimum Description Length.
    </a>
</span>

<br/>
<br/>
<span style="font-size:15px;">

    Probing classifiers often fail to adequately reflect  differences in representations
    and can show different results depending on hyperparameters.
    </br>
    As an alternative to the standard probes,

<ul>
    <li>we propose information-theoretic probing which measures
        <font face="arial">minimum description length</font> (MDL) of labels given representations;</li>
    <li>we show that MDL characterizes both <font face="arial">probe quality</font> and
      <font face="arial">the amount of effort</font> needed to achieve it;</li>
    <li>we explain how to easily measure MDL on top of standard probe-training pipelines;</li>
    <li>we show that results of MDL probes are more informative and stable than those of standard probes.</li>
</ul>
</span>

<a class="pull-right" href="/posts/mdl_probes.html" onMouseOver="document.readmore4.src='../resources/posts/buttons/button_read_more_push-min.png';" onMouseOut="document.readmore4.src='../resources/posts/buttons/button_read_more-min.png';">
<img src="../resources/posts/buttons/button_read_more-min.png" name="readmore4" width=120px class="pull-right"></a>
<a class="pull-right" href="https://arxiv.org/pdf/2003.12298.pdf" target="_blank" onMouseOver="document.readpaper4.src='../resources/posts/buttons/button_read_paper_push-min.png';" onMouseOut="document.readpaper4.src='../resources/posts/buttons/button_read_paper-min.png';">
<img src="../resources/posts/buttons/button_read_paper-min.png" name="readpaper4" width=120px class="pull-right"></a>
<a class="pull-right" href="https://github.com/lena-voita/description-length-probing" target="_blank" onMouseOver="document.viewcode4.src='../resources/posts/buttons/button_view_code_push-min.png';" onMouseOut="document.viewcode4.src='../resources/posts/buttons/button_view_code-min.png';">
<img src="../resources/posts/buttons/button_view_code-min.png" name="viewcode4" width=120px></a>

<span style="font-size:15px; text-align: right; float: right; color:gray">March 2020</span>

    </div>
</div>


<!-- ################################################################################### -->

<div class="fullCard" id="thumbnail" >
    <div class="cardContent">
        <h1 style="font-size:28px;">Evolution of Representations in the Transformer</h1>


<a class="float-right">
    <img src="../resources/posts/emnlp19_evolution/fugue_logo_on_white-min-min.png" alt="" style="max-width:350px; height:auto; float: right"/>
</a>


<span style="font-size:14px;">
This is a post for the EMNLP 2019 paper
    <a href="https://arxiv.org/abs/1909.01380" target="_blank">
            The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives.
    </a>
</span>


<br/>
<br/>
<span style="font-size:15px;">
We look at the evolution of representations of individual tokens in Transformers trained with different
    training objectives (MT, LM, MLM - BERT-style) from the
    <a href="https://www.cs.huji.ac.il/labs/learning/Papers/allerton.pdf">Information Bottleneck</a>
    perspective and show, that:
<ul>
  <li>LMs gradually forget past when forming predictions about future;</li>
  <li>for MLMs, the evolution proceeds in two stages of
      <font face="arial">context encoding</font> and <font face="arial">token reconstruction</font>;</li>
    <li>MT representations get refined with context,
        but less processing is happening.</li>
</ul>
</span>


<a class="pull-right" href="/posts/emnlp19_evolution.html" onMouseOver="document.readmore3.src='../resources/posts/buttons/button_read_more_push-min.png';" onMouseOut="document.readmore3.src='../resources/posts/buttons/button_read_more-min.png';">
<img src="../resources/posts/buttons/button_read_more-min.png" name="readmore3" width=120px class="pull-right"></a>
<a class="pull-right" href="https://arxiv.org/abs/1909.01380" target="_blank" onMouseOver="document.readpaper3.src='../resources/posts/buttons/button_read_paper_push-min.png';" onMouseOut="document.readpaper3.src='../resources/posts/buttons/button_read_paper-min.png';">
<img src="../resources/posts/buttons/button_read_paper-min.png" name="readpaper3" width=120px class="pull-right"></a>
<span style="font-size:15px; text-align: right; float: right; color:gray">September 2019</span>
    </div>
</div>


<!-- ################################################################################### -->


<div class="fullCard" id="thumbnail" >
    <div class="cardContent">
        <h1 style="font-size:28px;">When a Good Translation is Wrong in Context</h1>


<video width="380" height="auto" style="float: right" loop autoplay muted>
  <source src="../resources/posts/acl19_ctx/cadec_post_crop.mp4" type="video/mp4">
</video>


<span style="font-size:14px;">
This is a post for the ACL 2019 paper
    <a href="https://www.aclweb.org/anthology/P19-1116" target="_blank">
            When a Good Translation is Wrong in Context: Context-Aware Machine Translation Improves on Deixis, Ellipsis, and Lexical Cohesion.
    </a>
</span>


<br/>
<br/>
<span style="font-size:15px;">
From this post, you will learn:
<ul>
  <li>which phenomena cause context-agnostic translations to be inconsistent with each other</li>
  <li>how we create test sets addressing the most frequent phenomena</li>
    <li>about a novel set-up for context-aware NMT with a large amount of sentence-level data and
        much less of document-level data</li>
  <li>about a new model for this set-up (<font color="#CA6F1E">C</font>ontext-<font color="#CA6F1E">A</font>ware
      <font color="#CA6F1E">Dec</font>oder, aka <font color="#CA6F1E">CADec</font>) - a two-pass MT model which first produces a draft translation of the current sentence, then corrects it using context.</li>
</ul>
</span>


<a class="pull-right" href="/posts/acl19_context.html" onMouseOver="document.readmore2.src='../resources/posts/buttons/button_read_more_push-min.png';" onMouseOut="document.readmore2.src='../resources/posts/buttons/button_read_more-min.png';">
<img src="../resources/posts/buttons/button_read_more-min.png" name="readmore2" width=120px class="pull-right"></a>
<a class="pull-right" href="https://www.aclweb.org/anthology/P19-1116" target="_blank"onMouseOver="document.readpaper2.src='../resources/posts/buttons/button_read_paper_push-min.png';" onMouseOut="document.readpaper2.src='../resources/posts/buttons/button_read_paper-min.png';">
<img src="../resources/posts/buttons/button_read_paper-min.png" name="readpaper2" width=120px class="pull-right"></a>
<a class="pull-right" href="https://github.com/lena-voita/good-translation-wrong-in-context" target="_blank" onMouseOver="document.viewcode2.src='../resources/posts/buttons/button_view_code_push-min.png';" onMouseOut="document.viewcode2.src='../resources/posts/buttons/button_view_code-min.png';">
<img src="../resources/posts/buttons/button_view_code-min.png" name="viewcode2" width=120px></a>
<span style="font-size:15px; text-align: right; float: right; color:gray">July 2019</span>

    </div>
</div>


<!-- ################################################################################### -->


<div class="fullCard" id="thumbnail" >
    <div class="cardContent">
        <h1 style="font-size:28px;">The Story of Heads</h1>


<a class="float-right">
    <img src="../img/paper/acl19_heads-min.png" alt="" style="max-width:350px; height:auto; float: right"/>
</a>

<span style="font-size:14px;">
This is a post for the ACL 2019 paper
    <a href="https://www.aclweb.org/anthology/P19-1580" target="_blank">
            Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned.
    </a>
</span>

<br/>
<br/>
<span style="font-size:15px;">
From this post, you will learn:
<ul>
  <li>how we evaluate the importance of attention heads in Transformer</li>
  <li>which functions the most important encoder heads perform</li>
  <li>how we prune the vast majority of attention heads in Transformer without seriously affecting quality</li>
    <li>which types of model attention are most sensitive to the number of attention heads and on which layers </li>
</ul>
</span>


<a class="pull-right" href="/posts/acl19_heads.html" onMouseOver="document.readmore.src='../resources/posts/buttons/button_read_more_push-min.png';" onMouseOut="document.readmore.src='../resources/posts/buttons/button_read_more-min.png';">
<img src="../resources/posts/buttons/button_read_more-min.png" name="readmore" width=120px class="pull-right"></a>
<a class="pull-right" href="https://www.aclweb.org/anthology/P19-1580" target="_blank" onMouseOver="document.readpaper.src='../resources/posts/buttons/button_read_paper_push-min.png';" onMouseOut="document.readpaper.src='../resources/posts/buttons/button_read_paper-min.png';">
<img src="../resources/posts/buttons/button_read_paper-min.png" name="readpaper" width=120px class="pull-right"></a>
<a class="pull-right" href="https://github.com/lena-voita/the-story-of-heads" target="_blank" onMouseOver="document.viewcode.src='../resources/posts/buttons/button_view_code_push-min.png';" onMouseOut="document.viewcode.src='../resources/posts/buttons/button_view_code-min.png';">
<img src="../resources/posts/buttons/button_view_code-min.png" name="viewcode" width=120px></a>
<span style="font-size:15px; text-align: right; float: right; color:gray">June 2019</span>

    </div>
</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <table >
          <tr class="contact-list">


            <td>
              <div class="media">
                <a class="pull-left thumbnail" href="mailto:lena-voita@hotmail.com">
                  <img src="/img/ico/mail-min.png" height=20px alt=""/>
                </a>
              </div>
            </td>
            <td>
              <div class="media">
                <a class="pull-left thumbnail" href="https://scholar.google.com/citations?user=EcN9o7kAAAAJ">
                  <img src="/img/ico/gs-min-round.png" height=20px alt=""/>
                </a>
              </div>
            </td>
            <td>
              <div class="media">
                <a class="pull-left thumbnail" href="https://github.com/lena-voita">
                  <img src="/img/ico/git.png" height=20px alt=""/>
                </a>
              </div>
            </td>
            <td>
              <div class="media">
                <a class="pull-left thumbnail" href="https://twitter.com/lena_voita">
                  <img src="/img/ico/twitter-min.png" height=20px alt=""/>
                </a>
              </div>
            </td>

          </tr>
        </table>
      </div>

      <!--<div class="footer-col  footer-col-2">
      </div> -->

      <div class="footer-col  footer-col-3">
          <p class="text" align="right">Last updated June 11, 2025.</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
